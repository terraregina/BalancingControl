{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# Imports\n",
    "# \n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import chart_studio.plotly as py\n",
    "# import plotly.express as px\n",
    "import pandas as pd\n",
    "# import cufflinks as cf\n",
    "import json as js\n",
    "# cf.go_offline()\n",
    "# cf.set_config_file(offline=False, world_readable=True)\n",
    "from itertools import product, repeat\n",
    "import os\n",
    "import action_selection as asl\n",
    "from itertools import product\n",
    "import jsonpickle as pickle\n",
    "import jsonpickle.ext.numpy as jsonpickle_numpy\n",
    "import json\n",
    "import torch as ar\n",
    "import perception as prc\n",
    "import agent as agt\n",
    "from environment import PlanetWorld\n",
    "from agent import BayesianPlanner\n",
    "from world import World\n",
    "from planet_sequences import generate_trials_df\n",
    "import time\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing.pool as mpp\n",
    "import tqdm\n",
    "from run_exampe_habit_v1 import run_agent\n",
    "\n",
    "def istarmap(self, func, iterable, chunksize=1):\n",
    "    \"\"\"starmap-version of imap\n",
    "    \"\"\"\n",
    "    self._check_running()\n",
    "    if chunksize < 1:\n",
    "        raise ValueError(\n",
    "            \"Chunksize must be 1+, not {0:n}\".format(\n",
    "                chunksize))\n",
    "\n",
    "    task_batches = mpp.Pool._get_tasks(func, iterable, chunksize)\n",
    "    result = mpp.IMapIterator(self)\n",
    "    self._taskqueue.put(\n",
    "        (\n",
    "            self._guarded_task_generation(result._job,\n",
    "                                          mpp.starmapstar,\n",
    "                                          task_batches),\n",
    "            result._set_length\n",
    "        ))\n",
    "    return (item for chunk in result for item in chunk)\n",
    "\n",
    "\n",
    "mpp.Pool.istarmap = istarmap\n",
    "\n",
    "\n",
    "def run_single_sim(lst,\n",
    "                    ns,\n",
    "                    na,\n",
    "                    npl,\n",
    "                    nc,\n",
    "                    nr,\n",
    "                    T,\n",
    "                    state_transition_matrix,\n",
    "                    planet_reward_probs,\n",
    "                    planet_reward_probs_switched,\n",
    "                    repetitions, use_fitting):\n",
    "\n",
    "\n",
    "    switch_cues, contingency_degradation, reward_naive, context_trans_prob, cue_ambiguity, h,\\\n",
    "    training_blocks, degradation_blocks, trials_per_block, dec_temp, rew, util, config_folder = lst\n",
    "    \n",
    "    config = 'planning_config' + '_degradation_'+ str(int(contingency_degradation)) \\\n",
    "                      + '_switch_' + str(int(switch_cues))                \\\n",
    "                      + '_train' + str(training_blocks)                   \\\n",
    "                      + '_degr' + str(degradation_blocks)                 \\\n",
    "                      + '_n' + str(trials_per_block)+'.json'\n",
    "\n",
    "\n",
    "    folder = os.path.join(os.getcwd(),'config/' + config_folder)\n",
    "    file = open(os.path.join(folder,config))\n",
    "\n",
    "    task_params = js.load(file)                                                                                 \n",
    "    colors = np.asarray(task_params['context'])          # 0/1 as indicator of color\n",
    "    sequence = np.asarray(task_params['sequence'])       # what is the optimal sequence\n",
    "    starts = np.asarray(task_params['starts'])           # starting position of agent\n",
    "    planets = np.asarray(task_params['planets'])         # planet positions \n",
    "    trial_type = np.asarray(task_params['trial_type'])\n",
    "    blocks = np.asarray(task_params['block'])\n",
    "\n",
    "\n",
    "    nblocks = int(blocks.max()+1)         # number of blocks\n",
    "    trials = blocks.size                  # number of trials\n",
    "    block = task_params['trials_per_block']           # trials per block\n",
    " \n",
    "    meta = {\n",
    "        'trial_file' : config, \n",
    "        'trial_type' : trial_type,\n",
    "        'switch_cues': switch_cues == True,\n",
    "        'contingency_degradation' : contingency_degradation == True,\n",
    "        'learn_rew' : reward_naive == True,\n",
    "        'context_trans_prob': context_trans_prob,\n",
    "        'cue_ambiguity' : cue_ambiguity,\n",
    "        'h' : h,\n",
    "        'optimal_sequence' : sequence,\n",
    "        'blocks' : blocks,\n",
    "        'trials' : trials,\n",
    "        'nblocks' : nblocks,\n",
    "        'degradation_blocks': task_params['degradation_blocks'],\n",
    "        'training_blocks': task_params['training_blocks'],\n",
    "        'interlace': task_params['interlace'],\n",
    "        'contingency_degrdataion': task_params['contingency_degradation'],\n",
    "        'switch_cues': task_params['switch_cues'],\n",
    "        'trials_per_block': task_params['trials_per_block']\n",
    "    }\n",
    "\n",
    "    all_optimal_seqs = np.unique(sequence)                                                                            \n",
    "\n",
    "    # reward probabilities schedule dependent on trial and planet constelation\n",
    "    Rho = np.zeros([trials, nr, ns])\n",
    "\n",
    "    for i, pl in enumerate(planets):\n",
    "        if i >= block*meta['training_blocks'] and i < block*(meta['training_blocks'] + meta['degradation_blocks']) and contingency_degradation:\n",
    "            # print(i)\n",
    "            # print(pl)\n",
    "            Rho[i,:,:] = planet_reward_probs_switched[tuple([pl])].T\n",
    "        else:\n",
    "            Rho[i,:,:] = planet_reward_probs[tuple([pl])].T\n",
    "\n",
    "    # u = 0.99\n",
    "    # utility = np.array([(1-u)/2,(1-u)/2,u])\n",
    "\n",
    "    utility = np.array([float(u)/100 for u in util])\n",
    "    # print(utility)\n",
    "\n",
    "    if reward_naive==True:\n",
    "        reward_counts = np.ones([nr, npl, nc])\n",
    "    else:\n",
    "        reward_counts = np.tile(planet_reward_probs.T[:,:,np.newaxis]*5,(1,1,nc))+1\n",
    "\n",
    "    par_list = [h,                        \n",
    "                context_trans_prob,\n",
    "                cue_ambiguity,            \n",
    "                'avg',                    \n",
    "                Rho,                      \n",
    "                utility,                  \n",
    "                state_transition_matrix,  \n",
    "                planets,                  \n",
    "                starts,                   \n",
    "                colors,\n",
    "                reward_counts,\n",
    "                1,\n",
    "                dec_temp,\n",
    "                rew]\n",
    "\n",
    "    prefix = 'multiple_'\n",
    "\n",
    "    if use_fitting == True:\n",
    "        prefix += 'fitt_'\n",
    "    else:\n",
    "        prefix +='hier_'\n",
    "\n",
    "    if switch_cues == True:\n",
    "        prefix += 'switch1_'\n",
    "    else:\n",
    "        prefix +='switch0_'\n",
    "\n",
    "    if contingency_degradation == True:\n",
    "        prefix += 'degr1_'\n",
    "    else:\n",
    "        prefix += 'degr0_'\n",
    "\n",
    "    fname = prefix +'p' + str(cue_ambiguity) +'_learn_rew' + str(int(reward_naive == True)) + '_q' + str(context_trans_prob) + '_h' + str(h)+ '_' +\\\n",
    "    str(meta['trials_per_block']) +'_'+str(meta['training_blocks']) + str(meta['degradation_blocks']) +\\\n",
    "    '_dec' + str(dec_temp)+ '_rew' + str(rew) + '_u' +  '-'.join(util) + '_' + config_folder\n",
    " \n",
    "    fname +=  '_extinguish.json'\n",
    "\n",
    "    worlds = [run_agent(par_list, trials, T, ns , na, nr, nc, npl, added=[trial_type,sequence], use_fitting=use_fitting) for _ in range(repetitions)]\n",
    "    meta['trial_type'] = task_params['trial_type']\n",
    "    meta['optimal_sequence'] = task_params['sequence']\n",
    "\n",
    "    worlds.append(meta)\n",
    "\n",
    "   \n",
    "    fname = os.path.join(os.path.join(os.getcwd(),'temp'), fname)\n",
    "    jsonpickle_numpy.register_handlers()\n",
    "    pickled = pickle.encode(worlds)\n",
    "    with open(fname, 'w') as outfile:\n",
    "        json.dump(pickled, outfile)\n",
    "\n",
    "\n",
    "    return fname\n",
    "\n",
    "def pooled(arrays):\n",
    "    use_fitting = False\n",
    "    repetitions = 1\n",
    "    seed = 521312\n",
    "    np.random.seed(seed)\n",
    "    ar.manual_seed(seed)\n",
    "\n",
    "    lst = []\n",
    "    path = os.path.join(os.getcwd(),'temp')\n",
    "    existing_files = os.listdir(path)\n",
    "    # print(existing_files[:15])\n",
    "\n",
    "    for i in product(*arrays):\n",
    "        lst.append(list(i))\n",
    "\n",
    "\n",
    "    names = []\n",
    "\n",
    "    for li, l in enumerate(lst):\n",
    "        prefix = 'multiple_'\n",
    "\n",
    "        if use_fitting == True:\n",
    "            prefix += 'fitt_'\n",
    "        else:\n",
    "            prefix += 'hier_'\n",
    "\n",
    "        if l[0] == True:\n",
    "            prefix += 'switch1_'\n",
    "        else:\n",
    "            prefix +='switch0_'\n",
    "\n",
    "        if l[1] == True:\n",
    "            prefix += 'degr1_'\n",
    "        else:\n",
    "            prefix += 'degr0_'\n",
    "\n",
    "\n",
    "        l[11] = [str(entry) for entry in l[11]]\n",
    "        fname = prefix + 'p' + str(l[4])  +'_learn_rew' + str(int(l[2] == True))+ '_q' + str(l[3]) + '_h' + str(l[5]) + '_' +\\\n",
    "        str(l[8]) + '_' + str(l[6]) + str(l[7]) + \\\n",
    "        '_dec' + str(l[9]) +'_rew' + str(l[10]) + '_' + 'u'+  '-'.join(l[11]) + '_' + l[12]\n",
    "\n",
    "        if extinguish:\n",
    "            fname += '_extinguish.json'\n",
    "        else:\n",
    "            fname += '.json'\n",
    "        names.append([li, fname])\n",
    "\n",
    "    # print(names[:5])\n",
    "    missing_files = []\n",
    "    for name in names:\n",
    "        if not name[1] in existing_files:\n",
    "            # print(name)\n",
    "            missing_files.append(name[0])\n",
    "\n",
    "    lst = [lst[i] for i in missing_files]\n",
    "    print('simulations to run: ' + str(len(lst)))\n",
    "\n",
    "    ca = [ns, na, npl, nc, nr, T, state_transition_matrix, planet_reward_probs,\\\n",
    "        planet_reward_probs_switched,repetitions,use_fitting]\n",
    "\n",
    "    with Pool() as pool:\n",
    "\n",
    "        for _ in tqdm.tqdm(pool.istarmap(run_single_sim, zip(lst,\\\n",
    "                                                repeat(ca[0]),\\\n",
    "                                                repeat(ca[1]),\\\n",
    "                                                repeat(ca[2]),\\\n",
    "                                                repeat(ca[3]),\\\n",
    "                                                repeat(ca[4]),\\\n",
    "                                                repeat(ca[5]),\\\n",
    "                                                repeat(ca[6]),\\\n",
    "                                                repeat(ca[7]),\\\n",
    "                                                repeat(ca[8]),\\\n",
    "                                                repeat(ca[9]),\\\n",
    "                                                repeat(ca[10]))),\n",
    "                        total=len(lst)):\n",
    "            pass\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    data_folder = 'temp'\n",
    "\n",
    "\n",
    "    extinguish = True\n",
    "\n",
    "    na = 2                                           # number of unique possible actions\n",
    "    nc = 4                                           # number of contexts, planning and habit\n",
    "    nr = 3                                           # number of rewards\n",
    "    ns = 6                                           # number of unique travel locations\n",
    "    npl = 3\n",
    "    steps = 3                                        # numbe of decisions made in an episode\n",
    "    T = steps + 1                                    # episode length\n",
    "\n",
    "\n",
    "    planet_reward_probs = np.array([[0.95, 0   , 0   ],\n",
    "                                    [0.05, 0.95, 0.05],\n",
    "                                    [0,    0.05, 0.95]]).T    # npl x nr\n",
    "    planet_reward_probs_switched = np.array([[0   , 0    , 0.95],\n",
    "                                            [0.05, 0.95 , 0.05],\n",
    "                                            [0.95, 0.05 , 0.0]]).T \n",
    "    state_transition_matrix = np.zeros([ns,ns,na])\n",
    "    m = [1,2,3,4,5,0]\n",
    "    for r, row in enumerate(state_transition_matrix[:,:,0]):\n",
    "        row[m[r]] = 1\n",
    "    j = np.array([5,4,5,6,2,2])-1\n",
    "    for r, row in enumerate(state_transition_matrix[:,:,1]):\n",
    "        row[j[r]] = 1\n",
    "    state_transition_matrix = np.transpose(state_transition_matrix, axes= (1,0,2))\n",
    "    state_transition_matrix = np.repeat(state_transition_matrix[:,:,:,np.newaxis], repeats=nc, axis=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run first\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import chart_studio.plotly as py\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cufflinks as cf\n",
    "import json as js\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "import itertools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=False)\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook_connected'\n",
    "import os\n",
    "import action_selection as asl\n",
    "from itertools import product, repeat\n",
    "import jsonpickle as pickle\n",
    "import jsonpickle.ext.numpy as jsonpickle_numpy\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "import perception as prc\n",
    "import agent as agt\n",
    "from environment import PlanetWorld\n",
    "from agent import BayesianPlanner\n",
    "from world import World\n",
    "from planet_sequences import generate_trials_df\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "# functions\n",
    "def load_file_names(arrays, use_fitting=False):\n",
    "    lst = []\n",
    "    for i in product(*arrays):\n",
    "        lst.append(list(i))\n",
    "    \n",
    "    names = []\n",
    "    print('files to load: ' + str(len(lst)))\n",
    "    for li, l in enumerate(lst):\n",
    "\n",
    "        prefix = 'multiple_'\n",
    "        if use_fitting == True:\n",
    "            prefix += 'fitt_'\n",
    "        else:\n",
    "            prefix +='hier_'\n",
    "\n",
    "        if l[0] == True:\n",
    "            prefix += 'switch1_'\n",
    "        else:\n",
    "            prefix +='switch0_'\n",
    "\n",
    "        if l[1] == True:\n",
    "            prefix += 'degr1_'\n",
    "        else:\n",
    "            prefix += 'degr0_'\n",
    "\n",
    "        # fname = prefix + 'p' + str(l[4])  +'_learn_rew' + str(int(l[2] == True))+ '_q' + str(l[3]) + '_h' + str(l[5]) + '_' +\\\n",
    "        # str(l[8]) + '_' + str(l[6]) + str(l[7])+ '_dec' + str(l[9])\n",
    "        \n",
    "        l[11] = [str(entry) for entry in l[11]]\n",
    "        fname = prefix + 'p' + str(l[4])  +'_learn_rew' + str(int(l[2] == True))+ '_q' + str(l[3]) + '_h' + str(l[5]) + '_' +\\\n",
    "        str(l[8]) + '_' + str(l[6]) + str(l[7]) + \\\n",
    "        '_dec' + str(l[9]) +'_rew' + str(l[10]) + '_' + 'u'+  '-'.join(l[11]) + '_' + l[12]\n",
    "\n",
    "        # # print(len(l))\n",
    "        # if len(l) > 10:\n",
    "        #     fname += '_' + l[-1]\n",
    "\n",
    "        fname +=  '_extinguish.json'\n",
    "\n",
    "        names.append(fname)\n",
    "\n",
    "\n",
    "    return names\n",
    "\n",
    "def load_df(names,data_folder='data', extinguish=None):\n",
    "\n",
    "    if extinguish is None:\n",
    "        raise('did not specify if rewarded during extinction')\n",
    "    # if not just_simulated:\n",
    "    path = os.path.join(os.getcwd(),data_folder)\n",
    "    #     names = os.listdir(path)\n",
    "    for fi, f in enumerate(names):\n",
    "        names[fi] = os.path.join(path,f)\n",
    "        # print(names[fi])\n",
    "\n",
    "    dfs = [None]*len(names)\n",
    "\n",
    "    for f,fname in enumerate(names):\n",
    "        jsonpickle_numpy.register_handlers()\n",
    "        with open(fname, 'r') as infile:\n",
    "            data = json.load(infile)\n",
    "        worlds = pickle.decode(data)\n",
    "        meta = worlds[-1]\n",
    "        agents = [w.agent for w in worlds[:-1]]\n",
    "        perception = [w.agent.perception for w in worlds[:-1]]\n",
    "        nt = worlds[0].T\n",
    "        npl = perception[0].npl\n",
    "        nr = worlds[0].agent.nr\n",
    "        nc = perception[0].nc\n",
    "        nw = len(worlds[:-1])\n",
    "        ntrials = meta['trials']\n",
    "        learn_rew = np.repeat(meta['learn_rew'], ntrials*nw*nt)\n",
    "        switch_cues = np.repeat(meta['switch_cues'], ntrials*nw*nt)\n",
    "        contingency_degradation = np.repeat(meta['contingency_degradation'], ntrials*nw*nt)\n",
    "        tr_per_block = np.repeat(meta['trials_per_block'], ntrials*nw*nt)\n",
    "        ndb = np.repeat(meta['degradation_blocks'], ntrials*nw*nt)\n",
    "        ntb = np.repeat(meta['training_blocks'], ntrials*nw*nt)\n",
    "        post_dir_rewards = [a.posterior_dirichlet_rew for a in agents]\n",
    "        post_dir_rewards = [post[:,1:,:,:] for post in post_dir_rewards]\n",
    "        entropy_rewards = np.zeros([nw*ntrials*nt,nc])\n",
    "        extinguished = np.zeros(ntrials*nw*nt, dtype='int32')\n",
    "        extinguished[:] = int(extinguish == True)\n",
    "        prior_rewards = worlds[0].agent.perception.prior_rewards\n",
    "        utility_0 = np.repeat(prior_rewards[0], ntrials*nw*nt)\n",
    "        utility_1 = np.repeat(prior_rewards[1], ntrials*nw*nt)\n",
    "        utility_2 = np.repeat(prior_rewards[2], ntrials*nw*nt)\n",
    "        r_lambda = np.repeat(worlds[0].agent.perception.r_lambda, ntrials*nw*nt)\n",
    "\n",
    "        for ip, post in enumerate(post_dir_rewards):\n",
    "            post = post.sum(axis=3)                      # sum out planets\n",
    "            norm = post.sum(axis=2)                      # normalizing constants\n",
    "            reward_distributions = np.zeros(post.shape)\n",
    "\n",
    "            for r in range(nr):                          # normalize each reward \n",
    "                reward_distributions[:,:,r,:] = np.divide(post[:,:,r,:],norm)\n",
    "            entropy = np.zeros([ntrials, nt, nc])\n",
    "\n",
    "            for trl in range(ntrials):\n",
    "                for t in range(nt-1):\n",
    "                    prob = reward_distributions[trl,t,:,:].T\n",
    "                    # if prob.sum() == 0:\n",
    "                    #     print('problem')\n",
    "                    entropy[trl,t+1,:] = -(np.log(prob)*prob).sum(axis=1)\n",
    "\n",
    "            entropy[:,0,:] = None\n",
    "            entropy_rewards[ip*(ntrials*nt):(ip+1)*(ntrials*nt),:] = np.reshape(entropy, [ntrials*nt,nc])\n",
    "\n",
    "        entropy_context = np.zeros(ntrials*nt*nw)\n",
    "        post_context = [a.posterior_context for a in agents]\n",
    "\n",
    "        for ip, post in enumerate(post_context):\n",
    "            entropy = np.zeros([ntrials, nt])\n",
    "\n",
    "            for trl in range(ntrials):\n",
    "                entropy[trl,:] = -(np.log(post[trl,:])*post[trl,:]).sum(axis=1) \n",
    "            entropy_context[ip*(ntrials*nt):(ip+1)*(ntrials*nt)] = np.reshape(entropy, [ntrials*nt])\n",
    "\n",
    "        posterior_context = [agent.posterior_context for agent in agents]\n",
    "        observations = [w.observations for w in worlds[:-1]]\n",
    "        context_cues = worlds[0].environment.context_cues\n",
    "        policies = worlds[0].agent.policies\n",
    "        actions = [w.actions[:,:3] for w in worlds[:-1]] \n",
    "        true_optimal = np.tile(np.repeat(meta['optimal_sequence'],nt), nw)\n",
    "        cue = np.tile(np.repeat(context_cues, nt), nw)\n",
    "        ex_p = np.zeros(ntrials)\n",
    "        executed_policy = np.zeros(nw*ntrials,dtype='int32')\n",
    "        optimality = np.zeros(nw*ntrials)\n",
    "        chose_optimal = np.zeros(nw*ntrials)\n",
    "\n",
    "        for w in range(nw):\n",
    "            for pi, p in enumerate(policies):\n",
    "                inds = np.where( (actions[w][:,0] == p[0]) & (actions[w][:,1] == p[1]) & (actions[w][:,2] == p[2]) )[0]\n",
    "                ex_p[inds] = pi\n",
    "            executed_policy[w*ntrials:(w+1)*ntrials] = ex_p\n",
    "            ch_op = executed_policy[w*ntrials:(w+1)*ntrials] == meta['optimal_sequence']\n",
    "            chose_optimal[w*ntrials:(w+1)*ntrials] = ch_op\n",
    "            optimality[w*ntrials:(w+1)*ntrials] = np.cumsum(ch_op)/(np.arange(ntrials)+1)\n",
    "\n",
    "        executed_policy = np.repeat(executed_policy, nt)\n",
    "        chose_optimal = np.repeat(chose_optimal, nt)\n",
    "        optimality = np.repeat(optimality, nt)\n",
    "        no = perception[0].generative_model_context.shape[0]\n",
    "        optimal_contexts = [np.argmax(perception[0].generative_model_contexts[i,:] for i in range(no))]\n",
    "        true_context = 0\n",
    "        q = np.repeat(meta['context_trans_prob'], ntrials*nw*nt)\n",
    "        p = np.repeat(meta['cue_ambiguity'], ntrials*nw*nt)\n",
    "        h = np.repeat(meta['h'], ntrials*nw*nt)\n",
    "        dec_temp = np.repeat(worlds[0].dec_temp,ntrials*nw*nt)\n",
    "        switch_cues = np.repeat(meta['switch_cues'], ntrials*nw*nt)\n",
    "        learn_rew = np.repeat(meta['learn_rew'], ntrials*nw*nt)\n",
    "        degradation = np.repeat('contingency_degradation', ntrials*nw*nt)\n",
    "        trial_type = np.tile(np.repeat(meta['trial_type'], nt), nw)\n",
    "        trial = np.tile(np.repeat(np.arange(ntrials),nt), nw)\n",
    "        run = np.repeat(np.arange(nw),nt*ntrials)\n",
    "        run.astype('str')\n",
    "        inferred_context_t0 = np.zeros(ntrials*nw,dtype='int32')\n",
    "        inferred_context_t3  = np.zeros(ntrials*nw,'int32')\n",
    "        agnt = np.repeat(np.arange(nw)+f*nw,nt*ntrials)\n",
    "        true_context = np.zeros(ntrials*nw, dtype='int32')\n",
    "        no, nc = perception[0].generative_model_context.shape \n",
    "        modes_gmc =  perception[0].generative_model_context.argsort(axis=1)\n",
    "        contexts = [modes_gmc[i,:][-2:] for i in range(no)] # arranged in ascending order!\n",
    "        if_inferred_context_switch = np.zeros(ntrials, dtype=\"int32\")\n",
    "\n",
    "        # c is an array holding what contexts should be for the given task trials [0,1,0,1,..2,3,2,3...0,1] etc\n",
    "        to = np.zeros(ntrials, dtype=\"int32\")\n",
    "        for i in range(no):    \n",
    "            c = np.array([contexts[i][-1]]*(meta['trials_per_block']*meta['training_blocks'])\\\n",
    "                + [contexts[i][-2]]*(meta['trials_per_block']*meta['degradation_blocks'])\\\n",
    "                + [contexts[i][-1]]*meta['trials_per_block']*2)\n",
    "            to[np.where(context_cues == i)] = c[np.where(context_cues == i)]\n",
    "            # print(to)\n",
    "            if_inferred_context_switch[np.where(context_cues == i)] = c[np.where(context_cues == i)]\n",
    "        inferred_switch = np.zeros(ntrials*nw,dtype='int32')\n",
    "        context_optimality = np.zeros(ntrials*nw)\n",
    "\n",
    "        for w in range(nw):\n",
    "            inferred_context_t0[w*ntrials:(w+1)*ntrials] = np.argmax(posterior_context[w][:,0,:],axis=1)\n",
    "            inferred_context_t3[w*ntrials:(w+1)*ntrials] = np.argmax(posterior_context[w][:,-1,:],axis=1)\n",
    "            inferred_switch[w*ntrials:(w+1)*ntrials] = if_inferred_context_switch == \\\n",
    "                                                    inferred_context_t3[w*ntrials:(w+1)*ntrials]\n",
    "            context_optimality[w*ntrials:(w+1)*ntrials] = np.cumsum(inferred_switch[w*ntrials:(w+1)*ntrials])\\\n",
    "                                                                /(np.arange(ntrials)+1)\n",
    "        true_context[w*ntrials:(w+1)*ntrials] = to\n",
    "        inferred_switch = np.repeat(inferred_switch, nt)\n",
    "        inferred_context_t0 = np.repeat(inferred_context_t0, nt)\n",
    "        inferred_context_t3 = np.repeat(inferred_context_t3, nt)\n",
    "        context_optimality = np.repeat(context_optimality, nt)\n",
    "        true_context =  np.repeat(true_context, nt)\n",
    "        t = np.tile(np.arange(4), nw*ntrials)\n",
    "        \n",
    "        # print(true_context.size)\n",
    "        # print(trial_type.size)\n",
    "        d = {'trial_type':trial_type, 'run':run, 'trial':trial, 't':t, 'true_optimal':true_optimal,\\\n",
    "                            'cue':cue, 'q':q, 'p':p, 'h':h, 'inferred_context_t0':inferred_context_t0,\\\n",
    "                            'inferred_context_t3':inferred_context_t3, 'executed_policy':executed_policy,\\\n",
    "                            'chose_optimal': chose_optimal, 'entropy_rew_c1': entropy_rewards[:,0], 'entropy_rew_c2': entropy_rewards[:,1], \\\n",
    "                            'entropy_rew_c3': entropy_rewards[:,2] , 'entropy_rew_c4': entropy_rewards[:,3],\\\n",
    "                            'policy_optimality':optimality,'agent':agnt, 'inferred_switch': inferred_switch,\\\n",
    "                            'context_optimality':context_optimality, 'learn_rew': learn_rew, 'entropy_context':entropy_context, \\\n",
    "                            'switch_cues':switch_cues, 'contingency_degradation': contingency_degradation,\\\n",
    "                            'degradation_blocks': ndb, 'training_blocks':ntb, 'trials_per_block': tr_per_block,\\\n",
    "                            'true_context': true_context, 'dec_temp':dec_temp, 'utility_0': utility_0, 'utility_1': utility_1, 'utility_2': utility_2,\n",
    "                            'r_lambda': r_lambda} \n",
    "\n",
    "        # for key in d.keys():\n",
    "        #     print(key, np.unique(d[key].shape))\n",
    "        dfs[f] = pd.DataFrame(d)\n",
    "        \n",
    "    data = pd.concat(dfs)\n",
    "\n",
    "    groups = ['agent','run', 't','degradation_blocks', 'training_blocks', 'trials_per_block','learn_rew', 'p', 'q','h','cue']\n",
    "    grouped = data.groupby(by=groups)\n",
    "    data['iterator'] = 1\n",
    "    data['ith_cue_trial'] = grouped['iterator'].transform('cumsum')\n",
    "    data['policy_optimality_cue'] = grouped['chose_optimal'].transform('cumsum') / data['ith_cue_trial']\n",
    "    data['context_optimal_cue'] = grouped['inferred_switch'].transform('cumsum') / data['ith_cue_trial']\n",
    "    data.drop('iterator',1,inplace =True)\n",
    "    data.astype({'h': 'category'})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def context_plot(query='p == 0.6'):\n",
    "\n",
    "    # context and policy optimality\n",
    "    fig = plt.figure(figsize=(10,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    print('cue')\n",
    "    plot_df = base_df.query(query + '& t ==' + str(t) + ' & cue == ' + str(cue))\n",
    "    plot_df['h'] = plot_df['h'].astype('category')\n",
    "    ax = sns.lineplot(data=plot_df, x='trial', y='context_optimality', hue='h',\\\n",
    "                      palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size), legend=False)\n",
    "    ax.set(ylim = (0,1.1))\n",
    "    ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "    cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "    for i, row in ranges.iterrows():\n",
    "        ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "    # reward distribution entropy for different contexts\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = sns.lineplot(data=plot_df, x='trial', y='policy_optimality', hue='h',\\\n",
    "            palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size))\n",
    "    ax.legend(ncol = np.unique(plot_df['h']).size, bbox_to_anchor=(-2, -0.25), loc='upper left',\\\n",
    "              borderaxespad=0,title='h')\n",
    "    cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "    for i, row in ranges.iterrows():\n",
    "        ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "\n",
    "    ax.set(ylim = (0,1))\n",
    "    # title = base_query + ' & ' + query\n",
    "    # title = title.replace(' & ', ', ')\n",
    "    # title = title.replace('==', ':')\n",
    "    title = 'Inferred context and policy optimality for p: ' + query[-3:] + ', switch cues: '\\\n",
    "            + str(int(switch)) + ', degradation: ' + str(int(contingency_degr)) + \\\n",
    "            ', reward_naive: ' + str(int(reward_naive)) + ', cue_shown: ' + str(cue)\n",
    "    fig.suptitle(title, fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "def reward_entropy_plot(query='p == 0.6'):\n",
    "\n",
    "    # entropy of reward distribution for each context \n",
    "    plot_df = base_df.query(query + '& t ==' + str(t) + ' & cue == ' + str(cue))\n",
    "    plot_df['h'] = plot_df['h'].astype('category')\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(11, 7))\n",
    "    axs_flat = axs.flatten()\n",
    "    ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "\n",
    "    legs = [False, False, False, True]\n",
    "    ys = ['entropy_rew_c1', 'entropy_rew_c2','entropy_rew_c3','entropy_rew_c4',]\n",
    "    for c in range(nc):\n",
    "        sns.lineplot(ax=axs_flat[c], data=plot_df, x='trial',y=ys[c],hue='h',\\\n",
    "            palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size), legend=legs[c])\n",
    "        cols = [[1,1,1], [0,0,0],[1,1,1]]\n",
    "        for i, row in ranges.iterrows():\n",
    "            axs_flat[c].axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "        axs_flat[-1].legend(ncol = np.unique(plot_df['h']).size, bbox_to_anchor=(-2, -0.25), loc='upper left',\\\n",
    "                            borderaxespad=0.5,title='h')\n",
    "    \n",
    "    title = 'Reward distribution entropy for  p: ' + query[-3:] + ', switch cues: ' + str(int(switch)) +\\\n",
    "            ', degradation: ' + str(int(contingency_degr)) + ', reward_naive: ' + str(int(reward_naive)) + \\\n",
    "            ', cue_shown: ' + str(cue)\n",
    "    fig.suptitle(title, fontsize=15)\n",
    "\n",
    "def context_plot_cue_dependent(query='p == 0.6',print_counts=False,util = None):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,6), sharex = True)\n",
    "\n",
    "    lgnd = [False, True]\n",
    "    cues = [0,1]\n",
    "    titles_c = ['Inferred context optimality for cue 0','Inferred context optimality (at t4) for cue 1']\n",
    "    titles_p = ['Inferred policy optimality for cue 0','Inferred policy optimality for cue 1']\n",
    "\n",
    "    for cue in cues:\n",
    "        plot_df = base_df.query(query + '& t ==' + str(t) + ' & cue == ' + str(cue))\n",
    "\n",
    "        sns.lineplot(ax = axes[0,cue], data=plot_df, x='trial', y='context_optimal_cue', hue='h',\\\n",
    "                      palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size),legend=False)\n",
    "        axes[0,cue].set_title(titles_c[cue])\n",
    "        axes[1,cue].set_title(titles_p[cue])\n",
    "        sns.lineplot(ax=axes[1,cue], data=plot_df, x='trial', y='policy_optimality_cue', hue='h',\\\n",
    "            palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size), legend=lgnd[cue])\n",
    "    axes[1,1].legend(ncol = np.unique(plot_df['h']).size, bbox_to_anchor=(-2, -0.25), loc='upper left',\\\n",
    "              borderaxespad=0,title='h')\n",
    "\n",
    "    if print_counts:\n",
    "        test_df =  base_df.query(query + '& t ==' + str(t))\n",
    "        test_df['correct'] = test_df['inferred_context_t3']== test_df['true_context']\n",
    "        cols = ['agent','h', 'trial','trial_type', 'cue', 'inferred_context_t3', 'true_context','correct']\n",
    "        counts = test_df[cols].groupby(by=['h','trial_type','cue','correct']).size()\n",
    "        print(counts)\n",
    "\n",
    "    ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "    cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "    for ax in axes.flatten():\n",
    "        ax.set_ylim([0,1.05])\n",
    "        for i, row in ranges.iterrows():\n",
    "            ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "    \n",
    "\n",
    "\n",
    "    # title = base_query + ' & ' + query\n",
    "    # title = title.replace(' & ', ', ')\n",
    "    # title = title.replace('==', ':')\n",
    "    title = 'p: ' + query[-3:] + ', switch cues: '\\\n",
    "            + str(int(switch)) + ', degradation: ' + str(int(contingency_degr)) + \\\n",
    "            ', reward_naive: ' + str(int(reward_naive)) + ' ' + util\n",
    "    fig.suptitle(title, fontsize=15)\n",
    "\n",
    "\n",
    "def load_df_animation_context(names,data_folder='temp'):\n",
    "\n",
    "    path = os.path.join(os.getcwd(),data_folder)\n",
    "    for fi, f in enumerate(names):\n",
    "        names[fi] = os.path.join(path,f)\n",
    "\n",
    "\n",
    "    overall_df = [None for _ in range(len(names))]\n",
    "    for f,fname in enumerate(names):\n",
    "        jsonpickle_numpy.register_handlers()\n",
    "\n",
    "        with open(fname, 'r') as infile:\n",
    "            data = json.load(infile)\n",
    "\n",
    "        worlds = pickle.decode(data)\n",
    "        meta = worlds[-1]\n",
    "        nw = len(worlds[:-1])\n",
    "        agents = [w.agent for w in worlds[:-1]]\n",
    "        posterior_context = [agent.posterior_context for agent in agents]\n",
    "        ntrials, t, nc = posterior_context[0].shape\n",
    "        outcome_surprise = [agent.outcome_suprise for agent in agents]\n",
    "        # context_obs_surprise = [agent.context_obs_surprise for agent in agents]\n",
    "        policy_surprise = [agent.policy_surprise for agent in agents]\n",
    "        policy_entropy = [agent.policy_entropy for agent in agents]\n",
    "        npi = agents[0].posterior_policies.shape[2]\n",
    "        taus = np.arange(ntrials)\n",
    "        ts = np.arange(t)\n",
    "        cs = np.arange(nc)\n",
    "        pis_post = np.array(['post_' for _ in range(npi)], dtype=object) + np.array([str(i) for i in range(npi)], dtype=object)\n",
    "        pis_prior = np.array(['post_' for _ in range(npi)], dtype=object) + np.array([str(i) for i in range(npi)], dtype=object)\n",
    "        pis_like = np.array(['post_' for _ in range(npi)], dtype=object) + np.array([str(i) for i in range(npi)], dtype=object)\n",
    "\n",
    "        mi = pd.MultiIndex.from_product([taus, ts, cs], names=['trial', 't', 'context'])\n",
    "        mi_post = pd.MultiIndex.from_product([taus, ts, pis_post, cs], names=['trial', 't', 'policy','context'])\n",
    "        mi_like = pd.MultiIndex.from_product([taus, ts, pis_like, cs], names=['trial', 't', 'policy','context'])\n",
    "        mi_prior = pd.MultiIndex.from_product([taus, ts, pis_like, cs], names=['trial', 't', 'policy','context'])\n",
    "\n",
    "        dfs = [None for _ in range(nw)]\n",
    "        factor = ntrials*nc*t\n",
    "\n",
    "        for w in range(nw):\n",
    "\n",
    "            policy_post_df =  pd.Series(index=mi_post, data=agents[w].posterior_policies.flatten())\n",
    "            policy_post_df = policy_post_df.unstack(level='policy').reset_index()\n",
    "            # fix entropy calcs\n",
    "            policy_prob = policy_post_df.iloc[:,-8:].to_numpy()\n",
    "            policy_prob[policy_prob == 0] = 10**(-300)\n",
    "            entropy = -(policy_prob*np.log(policy_prob)).sum(axis=1)\n",
    "\n",
    "            prior_policies = np.tile(agents[w].prior_policies[:,np.newaxis,:,:], (1,t,1,1))\n",
    "            # print(np.all(prior_policies[:,0,:,:] == prior_policies[:,1,:,:] ))\n",
    "            policy_prior_df =  pd.Series(index=mi_prior, data=prior_policies.flatten())\n",
    "            policy_prior_df = policy_prior_df.unstack(level='policy').reset_index()\n",
    "\n",
    "            policy_like_df =  pd.Series(index=mi_like, data=agents[w].likelihood.flatten())\n",
    "            policy_like_df = policy_like_df.unstack(level='policy').reset_index()\n",
    "            \n",
    "            policy_entropy_df = pd.Series(index=mi,\\\n",
    "                data=policy_entropy[w].flatten()).reset_index().rename(columns = {0:'policy_entropy'})\n",
    "            \n",
    "            policy_surprise_df = pd.Series(index=mi,\\\n",
    "                data=policy_surprise[w].flatten()).reset_index().rename(columns = {0:'context_obs_surprise'})\n",
    "            outcome_surprise_df = pd.Series(index=mi,\\\n",
    "                data=outcome_surprise[w].flatten()).reset_index().rename(columns = {0:'outcome_surprise'})\n",
    "                \n",
    "            # break\n",
    "            df = pd.Series(index=mi, data=posterior_context[w].flatten())\n",
    "            df = df.reset_index().rename(columns = {0:'probability'})\n",
    "            df['context_obs_surprise'] = policy_surprise_df['context_obs_surprise']\n",
    "            df['outcome_surprise'] = outcome_surprise_df['outcome_surprise']\n",
    "            df['policy_entropy'] = policy_entropy_df['policy_entropy']\n",
    "            # df['policy_entropy'] = policy_entropy_df['policy_entropy']\n",
    "            df['learn_rew'] = np.repeat(meta['learn_rew'], factor)\n",
    "            df['switch_cues'] = np.repeat(meta['switch_cues'], factor)\n",
    "            df['contingency_degradation'] = np.repeat(meta['contingency_degradation'], factor)\n",
    "            df['trials_per_block'] = np.repeat(meta['trials_per_block'], factor)\n",
    "            df['degradation_blocks'] = np.repeat(meta['degradation_blocks'], factor)\n",
    "            df['training_blocks'] = np.repeat(meta['training_blocks'], factor)\n",
    "            df['context_cues'] = np.repeat(worlds[0].environment.context_cues, nc*t)\n",
    "            df['true_optimal'] = np.repeat(meta['optimal_sequence'], nc*t)\n",
    "            df['q'] = np.repeat(meta['context_trans_prob'], factor)\n",
    "            df['p'] = np.repeat(meta['cue_ambiguity'], factor)\n",
    "            df['h'] = np.repeat(meta['h'], factor)\n",
    "            df['run'] = np.repeat(w,factor)\n",
    "            df['trial_type'] = np.repeat(meta['trial_type'], nc*t)\n",
    "            df['trial'] = np.repeat(np.arange(ntrials), nc*t)\n",
    "            df['agent'] = np.repeat(w+f*nw,factor)\n",
    "            df = df.join(policy_post_df.iloc[:,-8:])\n",
    "            dfs[w] = df\n",
    "            break\n",
    "        overall_df[f] = pd.concat(dfs)\n",
    "    data_animation = pd.concat(overall_df)\n",
    "    # data_animation.to_excel('data_animation.xlsx')\n",
    "    return data_animation\n",
    "# data_animation.to_csv('data_animation.csv')\n",
    "\n",
    "\n",
    "def load_df_animation_pol(names,data_folder='temp'):\n",
    "\n",
    "    path = os.path.join(os.getcwd(),data_folder)\n",
    "    #     names = os.listdir(path)\n",
    "\n",
    "    for fi, f in enumerate(names):\n",
    "        names[fi] = os.path.join(path,f)\n",
    "\n",
    "\n",
    "    overall_df = [None for _ in range(len(names))]\n",
    "    for f,fname in enumerate(names):\n",
    "        jsonpickle_numpy.register_handlers()\n",
    "\n",
    "        with open(fname, 'r') as infile:\n",
    "            data = json.load(infile)\n",
    "\n",
    "        worlds = pickle.decode(data)\n",
    "        meta = worlds[-1]\n",
    "        nw = len(worlds[:-1])\n",
    "        agents = [w.agent for w in worlds[:-1]]\n",
    "        posterior_context = [agent.posterior_context for agent in agents]\n",
    "        ntrials, t, nc = posterior_context[0].shape\n",
    "        policy_entropy = [agent.policy_entropy for agent in agents]\n",
    "        npi = agents[0].posterior_policies.shape[2]\n",
    "        taus = np.arange(ntrials)\n",
    "        ts = np.arange(t)\n",
    "        cs = np.arange(nc)\n",
    "        pis = np.arange(npi)\n",
    "\n",
    "        mi = pd.MultiIndex.from_product([taus, ts, pis, cs], names=['trial', 't','policy', 'context'])\n",
    "        dfs = [None for _ in range(nw)]\n",
    "        factor = ntrials*nc*t*npi\n",
    "\n",
    "        for w in range(nw):\n",
    "            df =  pd.Series(index=mi, data=agents[w].posterior_policies.flatten())\n",
    "            df = df.reset_index().rename(columns = {0:'policy_post'})\n",
    "\n",
    "            prior_policies = np.tile(agents[w].prior_policies[:,np.newaxis,:,:], (1,t,1,1))\n",
    "            policy_prior_df =  pd.Series(index=mi, data=prior_policies.flatten())\n",
    "            policy_prior_df = policy_prior_df.reset_index().rename(columns = {0:'policy_prior'})\n",
    "\n",
    "            policy_like_df =  pd.Series(index=mi, data=agents[w].likelihood.flatten())\n",
    "            policy_like_df = policy_like_df.reset_index().rename(columns = {0:'policy_likelihood'})\n",
    "            df['policy_prior'] = policy_prior_df['policy_prior']\n",
    "            df['policy_like'] = policy_like_df['policy_likelihood']\n",
    "            post_context = np.tile(posterior_context[w][:,:,np.newaxis,:], (1,1,npi,1))\n",
    "            # print(np.all(post_context[:,:,0,:] == post_context[:,:,3,:]))\n",
    "\n",
    "            post_context_df = pd.Series(index=mi, data=post_context.flatten())\n",
    "            post_context_df = post_context_df.reset_index().rename(columns = {0:'post_context'})\n",
    "            df['post_context'] = post_context_df['post_context']\n",
    "            df['learn_rew'] = np.repeat(meta['learn_rew'], factor)\n",
    "            df['switch_cues'] = np.repeat(meta['switch_cues'], factor)\n",
    "            df['contingency_degradation'] = np.repeat(meta['contingency_degradation'], factor)\n",
    "            df['trials_per_block'] = np.repeat(meta['trials_per_block'], factor)\n",
    "            df['degradation_blocks'] = np.repeat(meta['degradation_blocks'], factor)\n",
    "            df['training_blocks'] = np.repeat(meta['training_blocks'], factor)\n",
    "            df['context_cues'] = np.repeat(worlds[0].environment.context_cues, nc*t*npi)\n",
    "            df['true_optimal'] = np.repeat(meta['optimal_sequence'], nc*t*npi)\n",
    "            df['q'] = np.repeat(meta['context_trans_prob'], factor)\n",
    "            df['p'] = np.repeat(meta['cue_ambiguity'], factor)\n",
    "            df['h'] = np.repeat(meta['h'], factor)\n",
    "            df['run'] = np.repeat(w,factor)\n",
    "            df['trial_type'] = np.repeat(meta['trial_type'], nc*t*npi)\n",
    "            df['trial'] = np.repeat(np.arange(ntrials), nc*t*npi)\n",
    "            df['agent'] = np.repeat(w+f*nw,factor)\n",
    "            dfs[w] = df\n",
    "\n",
    "        overall_df[f] = pd.concat(dfs)\n",
    "    data_animation = pd.concat(overall_df)\n",
    "    # data_animation.to_excel('data_animation.xlsx')\n",
    "    return data_animation\n",
    "\n",
    "\n",
    "def load_df_reward_dkl(names,data_folder='temp',nc=4):\n",
    "\n",
    "    path = os.path.join(os.getcwd(),data_folder)\n",
    "    #     names = os.listdir(path)\n",
    "\n",
    "    for fi, f in enumerate(names):\n",
    "        names[fi] = os.path.join(path,f)\n",
    "\n",
    "    dfs = [None]*len(names)\n",
    "\n",
    "    planet_reward_probs = np.array([[0.95, 0   , 0   ],\n",
    "                            [0.05, 0.95, 0.05],\n",
    "                            [0,    0.05, 0.95]])  \n",
    "\n",
    "    planet_reward_probs_switched = np.array([[0   , 0    , 0.95],\n",
    "                                            [0.05, 0.95 , 0.05],\n",
    "                                            [0.95, 0.05 , 0.0]])\n",
    "    \n",
    "    planet_reward_probs = np.tile(planet_reward_probs[:,:,np.newaxis], (1,1,nc))\n",
    "    planet_reward_probs_switched = np.tile(planet_reward_probs_switched[:,:,np.newaxis], (1,1,nc))\n",
    "\n",
    "    overall_df = [None for _ in range(len(names))]\n",
    "\n",
    "    for f,fname in enumerate(names):\n",
    "        jsonpickle_numpy.register_handlers()\n",
    "        with open(fname, 'r') as infile:\n",
    "            data = json.load(infile)\n",
    "                         \n",
    "        worlds = pickle.decode(data)\n",
    "        meta = worlds[-1]\n",
    "        agents = [w.agent for w in worlds[:-1]]\n",
    "        perception = [w.agent.perception for w in worlds[:-1]]\n",
    "        reward_probs = [agent.posterior_dirichlet_rew for agent in agents]\n",
    "        prior_rewards = worlds[0].agent.perception.prior_rewards\n",
    "\n",
    "        nt = worlds[0].T\n",
    "        npl = perception[0].npl\n",
    "        nr = worlds[0].agent.nr\n",
    "        nc = perception[0].nc\n",
    "        nw = len(worlds[:-1])\n",
    "        ntrials = meta['trials']\n",
    "        # learn_rew = np.repeat(meta['learn_rew'], ntrials*nw*nt)\n",
    "\n",
    "        # switch_cues = np.repeat(meta['switch_cues'], ntrials*nw*nt)\n",
    "        # contingency_degradation = np.repeat(meta['contingency_degradation'], ntrials*nw*nt)\n",
    "        # ntrials_df = np.repeat(meta['trials_per_block'], ntrials*nw*nt)\n",
    "        # ndb = np.repeat(meta['degradation_blocks'], ntrials*nw*nt)\n",
    "        # ntb = np.repeat(meta['training_blocks'], ntrials*nw*nt)\n",
    "        post_dir_rewards = [a.posterior_dirichlet_rew for a in agents]\n",
    "        post_dir_rewards = [post[:,1:,:,:] for post in post_dir_rewards]\n",
    "        # entropy_rewards = np.zeros([nw*ntrials*nt,nc])\n",
    "        # extinguished = np.zeros(ntrials*nw*nt, dtype='int32')\n",
    "        # extinguished[:] = int(extinguish == True)\n",
    "        \n",
    "        # define true distribution reward\n",
    "        # if f == 0:\n",
    "        tpb = meta['trials_per_block']\n",
    "        db = meta['degradation_blocks']\n",
    "        tb = meta['training_blocks']\n",
    "        p= np.tile(planet_reward_probs[np.newaxis,np.newaxis,:,:,:], (ntrials, nt, 1,1,1))\n",
    "        p[tb*tpb:(tb+db)*tpb,:,:,:,:] = \\\n",
    "            np.tile(planet_reward_probs_switched[np.newaxis,np.newaxis,:,:,:],\n",
    "                    ((db + tb)*tpb - tb*tpb, nt, 1,1,1))\n",
    "        p[p == 0] = 10**(-300)\n",
    "        # else:\n",
    "        #     pass\n",
    "\n",
    "        factor = ntrials*nt*nc\n",
    "        taus = np.arange(ntrials)\n",
    "        ts = np.arange(nt)\n",
    "        # npls = np.char.add(np.asarray(['pl_' for _ in range(npl)]), np.asarray([str(i) for i in range(npl)]))\n",
    "        npls = np.arange(npl)\n",
    "        nrs = np.arange(nr)\n",
    "        cs = np.arange(nc)\n",
    "        mi = pd.MultiIndex.from_product([taus, ts, npls, cs],\n",
    "                names=['trial', 't', 'planet', 'context'])\n",
    "        # dfs_dkl = [None for _ in range(nw)]\n",
    "        # factor = ntrials*nt*nr*npl*nc\n",
    "\n",
    "        dkl_df = [None for _ in range(nw)]\n",
    "        for w in range(nw):\n",
    "            q = reward_probs[w]\n",
    "            e = (db+tb)*tpb\n",
    "            q[e:,:,:,:,:] = np.tile(q[e-1,:,:,:,:], (2*tpb,1,1,1,1))\n",
    "            q[q == 0] = 10**(-300)\n",
    "            norm = 1/(q.sum(axis=2))\n",
    "            q = np.einsum('etrpc, etpc -> etrpc', q, norm)\n",
    "            dkl = (q*np.log(q/p)).sum(axis=2)\n",
    "            df =  pd.Series(index=mi, data=dkl.flatten())\n",
    "            df = df.unstack(level = 'planet')\n",
    "            df = df.reset_index().rename(columns = {0:'p0_dkl', 1:'p1_dkl', 2:'p2_dkl'})\n",
    "            df['avg_dkl'] = (df['p0_dkl'] + df['p1_dkl'] + df['p2_dkl'])/3\n",
    "            df['learn_rew'] = np.repeat(meta['learn_rew'], factor)\n",
    "            df['switch_cues'] = np.repeat(meta['switch_cues'], factor)\n",
    "            df['contingency_degradation'] = np.repeat(meta['contingency_degradation'], factor)\n",
    "            df['trials_per_block'] = np.repeat(meta['trials_per_block'], factor)\n",
    "            df['degradation_blocks'] = np.repeat(meta['degradation_blocks'], factor)\n",
    "            df['training_blocks'] = np.repeat(meta['training_blocks'], factor)\n",
    "            df['context_cues'] = np.repeat(worlds[0].environment.context_cues, nc*nt)\n",
    "            df['true_optimal'] = np.repeat(meta['optimal_sequence'], nc*nt)\n",
    "            df['q'] = np.repeat(meta['context_trans_prob'], factor)\n",
    "            df['p'] = np.repeat(meta['cue_ambiguity'], factor)\n",
    "            df['dec_temp'] = np.repeat(worlds[0].dec_temp,factor)\n",
    "            df['h'] = np.repeat(meta['h'], factor)\n",
    "            df['run'] = np.repeat(w,factor)\n",
    "            df['trial_type'] = np.repeat(meta['trial_type'], nc*nt)\n",
    "            df['trial'] = np.repeat(np.arange(ntrials), nc*nt)\n",
    "            df['agent'] = np.repeat(w+f*nw,factor)\n",
    "            df['utility_0'] = np.repeat(prior_rewards[0], factor)\n",
    "            df['utility_1'] = np.repeat(prior_rewards[1], factor)\n",
    "            df['utility_2'] = np.repeat(prior_rewards[2], factor)\n",
    "            df['r_lambda'] = np.repeat(worlds[0].agent.perception.r_lambda, factor)\n",
    "            dkl_df[w] = df\n",
    "        #     break\n",
    "        # break\n",
    "        overall_df[f] = pd.concat(dkl_df)\n",
    "    data = pd.concat(overall_df)\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Load Data  <a class=\"anchor\" id=\"section_3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(os.path.join(os.getcwd(),'config/explore/config_degradation_1_switch_0_train4_degr4_n70.json'))\n",
    "# data = json.load(f)\n",
    "# df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####OLD \n",
    "nc = 4\n",
    "extinguish = True\n",
    "\n",
    "\n",
    "h =  [1, 100]\n",
    "cue_ambiguity = [0.9,0.98]                       \n",
    "context_trans_prob = [0.9]                \n",
    "degradation = [True]\n",
    "cue_switch = [False]\n",
    "reward_naive = [True, False]\n",
    "training_blocks = [4]\n",
    "degradation_blocks=[6]\n",
    "trials_per_block=[70]\n",
    "dec_temp = [1,4]\n",
    "conf= ['explore']\n",
    "utility = [[1, 19, 80], [0.5,0.5,99]]#,[5,25,70],[1, 9, 90]]\n",
    "\n",
    "\n",
    "arrays = [cue_switch, degradation, reward_naive, context_trans_prob, cue_ambiguity,h,\\\n",
    "        training_blocks, degradation_blocks, trials_per_block,dec_temp,utility, conf]\n",
    "\n",
    "\n",
    "names = load_file_names(arrays)\n",
    "# print(names)\n",
    "# df = load_df(names, data_folder='temp/old',extinguish=extinguish)\n",
    "df = load_df(names, data_folder='temp',extinguish=extinguish)\n",
    "df_dkl = load_df_reward_dkl(names, data_folder='temp')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 4\n",
    "extinguish = True\n",
    "\n",
    "h =  [1,2,3,4,10,50,100]\n",
    "# h = [40]\n",
    "cue_ambiguity = [0.65,0.7,0.75,0.8,0.85,0.9]                       \n",
    "context_trans_prob = [0.75, 0.8,0.85,0.9]\n",
    "cue_switch = [False]\n",
    "reward_naive = [True]\n",
    "training_blocks = [4]\n",
    "degradation_blocks=[6]\n",
    "degradation = [True]\n",
    "trials_per_block=[70]\n",
    "dec_temps = [1,2,7]\n",
    "# utility = [[1, 19, 80]]\n",
    "rews = [0]\n",
    "utility = [[5,25,70],[1,1,98],[1, 9, 90],[1, 19, 80]]\n",
    "conf = ['shuffled_and_blocked', 'shuffled']\n",
    "\n",
    "\n",
    "\n",
    "arrays = [cue_switch, degradation, reward_naive, context_trans_prob, cue_ambiguity,h,\\\n",
    "        training_blocks, degradation_blocks, trials_per_block,dec_temps,rews, utility, conf]\n",
    "# data_folder ='temp'\n",
    "pooled(arrays)\n",
    "# names = load_file_names(arrays)\n",
    "# # print(names)\n",
    "# # df = load_df(names, data_folder='temp/old',extinguish=extinguish)\n",
    "# df = load_df(names, data_folder='temp',extinguish=extinguish)\n",
    "# df_dkl = load_df_reward_dkl(names, data_folder='temp')\n",
    "# df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context optimality and choice optimality plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch = False\n",
    "contingency_degr = True\n",
    "reward_naive = True\n",
    "\n",
    "\n",
    "q = context_trans_prob[0]\n",
    "h = 200\n",
    "t = 3\n",
    "trials_per_block = 70\n",
    "training_blocks = 4\n",
    "# degradation_blocks = [2,4,6]\n",
    "degradation_blocks = [6]\n",
    "queries =  ['p==' + str(cue_ambiguity[0])]\n",
    "cue = 0\n",
    "dec_temp = 1\n",
    "# util = utility[0]\n",
    "# util = [u/100 for u in util]\n",
    "# print(util)\n",
    "one_run = True\n",
    "rew = rews[0]\n",
    "db = degradation_blocks[0]\n",
    "\n",
    "# for db in degradation_blocks:\n",
    "for util in utility:\n",
    "    \n",
    "    util = [u/100 for u in util]\n",
    "    # print(cue_ambiguity[ind], q,util)\n",
    "    strs = np.array(['switch_cues==', '& contingency_degradation==', '& learn_rew==', '& q==', '& h<=',\\\n",
    "                    '& training_blocks==', '& degradation_blocks==', '& trials_per_block==', '& dec_temp ==',\\\n",
    "                    '& utility_0==', '& utility_1==', '& utility_2==', '& r_lambda=='],dtype='str')\n",
    "    vals = np.array([switch, contingency_degr, reward_naive, q, h,\\\n",
    "                    training_blocks, db, trials_per_block, dec_temp, util[0], util[1], util[2],rew], dtype='str')\n",
    "    whole_query = np.char.join('', np.char.add(strs, vals))\n",
    "    base_query = ' '.join(whole_query.tolist())\n",
    "    if one_run == True:\n",
    "        base_query += ' & run == 0'\n",
    "\n",
    "    base_df = df.query(base_query)\n",
    "    \n",
    "    # print(base_df.size) # base_df_dkl = df_dkl.query(base_query)\n",
    "    # queries = ['p==0.95','p==0.7']\n",
    "    p = queries[0]\n",
    "    # print(base_query)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    context_plot_cue_dependent(p,util = str(util))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 4\n",
    "extinguish = True\n",
    "\n",
    "\n",
    "h =  [1,2,3,4,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "h = [40]\n",
    "cue_ambiguity = [0.9]                       \n",
    "context_trans_prob = [0.85]\n",
    "cue_switch = [False]\n",
    "reward_naive = [True]\n",
    "training_blocks = [4]\n",
    "degradation_blocks=[6]\n",
    "degradation = [True]\n",
    "trials_per_block=[70]\n",
    "dec_temps = [1]\n",
    "rews = [0]\n",
    "utility = [[1, 19, 80]]\n",
    "\n",
    "conf = ['explore']\n",
    "\n",
    "\n",
    "arrays = [cue_switch, degradation, reward_naive, context_trans_prob, cue_ambiguity,h,\\\n",
    "        training_blocks, degradation_blocks, trials_per_block,dec_temps, rews, utility, conf]\n",
    "\n",
    "pooled(arrays)\n",
    "names = load_file_names(arrays)\n",
    "# print(names)\n",
    "# df = load_df(names, data_folder='temp/old',extinguish=extinguish)\n",
    "df = load_df(names, data_folder='temp',extinguish=extinguish)\n",
    "df_dkl = load_df_reward_dkl(names, data_folder='temp')\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "utility = [[1, 19, 80],[1, 19, 80]]\n",
    "switch = False\n",
    "contingency_degr = True\n",
    "reward_naive = True\n",
    "q = context_trans_prob[0]\n",
    "h = 200\n",
    "t = 3\n",
    "trials_per_block = 70\n",
    "# p = cue_ambiguity[0]\n",
    "training_blocks = 4\n",
    "degradation_blocks = [6]\n",
    "dec_temp = 1\n",
    "one_run = True\n",
    "rew=0\n",
    "sns.set_style('whitegrid')\n",
    "for p in cue_ambiguity:\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=len(utility), figsize = (len(utility)*5,7))\n",
    "    fig.suptitle('p=' + str(p))\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    # for di, db in enumerate(degradation_blocks):\n",
    "    for ui, util in enumerate(utility):\n",
    "        db = degradation_blocks[0]\n",
    "        util = [u/100 for u in util]\n",
    "        # print(util)\n",
    "        strs = np.array(['switch_cues==', '& contingency_degradation==', '& learn_rew==', '& q==', '& h<=', '& p==',\\\n",
    "                        '& training_blocks==', '& degradation_blocks==', '& trials_per_block==', '& dec_temp ==',\\\n",
    "                        '& utility_0==', '& utility_1==', '& utility_2==', '& r_lambda=='],dtype='str')\n",
    "                        \n",
    "        vals = np.array([switch, contingency_degr, reward_naive, q, h, p, \\\n",
    "                        training_blocks, db, trials_per_block, dec_temp, util[0], util[1], util[2],rew], dtype='str')\n",
    "        whole_query = np.char.join('', np.char.add(strs, vals))\n",
    "\n",
    "        # strs = np.array(['switch_cues==', '& contingency_degradation==', '& learn_rew==', '& q==', '& h<=',\\\n",
    "        #                 '& training_blocks==', '& degradation_blocks==', '& trials_per_block==', '& dec_temp =='],dtype='str')\n",
    "        # vals = np.array([switch, contingency_degr, reward_naive, q, h,\\\n",
    "        #                 training_blocks, db, trials_per_block, dec_temp], dtype='str')\n",
    "        # whole_query = np.char.join('', np.char.add(strs, vals))\n",
    "        base_query = ' '.join(whole_query.tolist())\n",
    "        # print(base_query)\n",
    "        if one_run == True:\n",
    "            base_query += ' & run == 0'\n",
    "\n",
    "        base_df = df.query(base_query)\n",
    "        base_df = base_df.astype({'h': 'category'})\n",
    "        plot_df = base_df.query('trial_type == 1  & t==0 & degradation_blocks == ' + str(db))\n",
    "        grouped = plot_df.groupby(by=['agent', 'run','h','cue'])\n",
    "        plot_df['policy_optimality_subset'] = grouped['chose_optimal'].transform('cumsum')\n",
    "        plot_df['offset'] = grouped['ith_cue_trial'].transform('min')\n",
    "        plot_df['policy_optimality_subset'] = plot_df['policy_optimality_subset'] / (plot_df['ith_cue_trial'] - plot_df['offset']+1)\n",
    "\n",
    "\n",
    "\n",
    "        for cue in [0,1]:\n",
    "            sns.lineplot(ax = axes[cue,ui], data=plot_df.query('cue ==' + str(cue)),\\\n",
    "                        x = 'trial',y='policy_optimality_subset', hue='h', legend=False,\\\n",
    "                        palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size))\n",
    "            axes[cue,ui].set_title('cue=' + str(cue) + ', ' + str(util))\n",
    "\n",
    "\n",
    "        ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "        cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "        for ax in axes.flatten():\n",
    "            ax.set_ylim([0,1])\n",
    "            for i, row in ranges.iterrows():\n",
    "                ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "\n",
    "        ax.legend(bbox_to_anchor=(-2,-0.3), loc='upper left', borderaxespad=0,title='h',ncol=np.unique(plot_df['h'].size))\n",
    "\n",
    "        # fig.suptitle(ttls[f] + ' and cue_degradation: ' + str(cue_degradation[ci]) , fontsize=15, y=1.08)   \n",
    "    # plot_df.to_excel(str(db) + '.xlsx')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch = False\n",
    "# contingency_degr = True\n",
    "# reward_naive = True\n",
    "# q = cue_ambiguity[0]\n",
    "# h = 200\n",
    "# t = 3\n",
    "# trials_per_block = 70\n",
    "# training_blocks = 4\n",
    "# degradation_blocks = [2,4,6]\n",
    "# queries =  ['p==' + str(cue_ambiguity[0])]\n",
    "# cue = 0\n",
    "# dec_temp = 4\n",
    "# one_run = True\n",
    "\n",
    "\n",
    "# for di, db in enumerate(degradation_blocks):\n",
    "#     strs = np.array(['switch_cues==', '& contingency_degradation==', '& learn_rew==', '& q==', '& h<=',\\\n",
    "#                     '& training_blocks==', '& degradation_blocks==', '& trials_per_block==', '& dec_temp =='],dtype='str')\n",
    "#     vals = np.array([switch, contingency_degr, reward_naive, q, h,\\\n",
    "#                     training_blocks, db, trials_per_block, dec_temp], dtype='str')\n",
    "#     whole_query = np.char.join('', np.char.add(strs, vals))\n",
    "#     base_query = ' '.join(whole_query.tolist())\n",
    "\n",
    "#     if one_run == True:\n",
    "#         base_query += ' & run == 0'\n",
    "\n",
    "#     base_df = df.query(base_query)\n",
    "#     # base_df_dkl = df_dkl.query(base_query)\n",
    "#     # queries = ['p==0.95','p==0.7']\n",
    "#     p = queries[0]\n",
    "\n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=2, figsize = (10,3))\n",
    "#     plt.tight_layout()\n",
    "#     plot_df = base_df.query('t == ' + str(t) + ' & degradation_blocks ==' + str(db))\n",
    "#     for cue in [0,1]:\n",
    "#         sns.lineplot(ax = axes[cue], data=plot_df,\\\n",
    "#                     x = 'trial',y='entropy_context', hue='h', legend=False)\n",
    "#         axes[cue].set_title('db: ' + str(db) + ' cue: ' + str(cue))\n",
    "\n",
    "\n",
    "#     ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "#     cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "#     for ax in axes.flatten():\n",
    "#         # ax.set_ylim([0,0.6])\n",
    "#         for i, row in ranges.iterrows():\n",
    "#             ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "\n",
    "#     ax.legend(bbox_to_anchor=(-2,-0.3), loc='upper left', borderaxespad=0,title='h',ncol=np.unique(plot_df['h'].size))\n",
    "#     # fig.suptitle(ttls[f] + ' and cue_degradation: ' + str(cue_degradation[ci]) , fontsize=15, y=1.08)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DKL plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "\n",
    "nc = 4\n",
    "extinguish = True\n",
    "\n",
    "h =  [1,2,3,4,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "h = [40]\n",
    "cue_ambiguity = [0.9]                       \n",
    "context_trans_prob = [0.9]\n",
    "cue_switch = [False]\n",
    "reward_naive = [True]\n",
    "training_blocks = [4]\n",
    "degradation_blocks=[6]\n",
    "degradation = [True]\n",
    "trials_per_block=[70]\n",
    "dec_temps = [1]\n",
    "utility = [[1, 19, 80]]\n",
    "rew = [0.0005]\n",
    "# utility = [[5,25,70],[1,1,98],[1, 9, 90],[1, 19, 80]]\n",
    "\n",
    "conf = ['explore']\n",
    "\n",
    "\n",
    "\n",
    "arrays = [cue_switch, degradation, reward_naive, context_trans_prob, cue_ambiguity,h,\\\n",
    "        training_blocks, degradation_blocks, trials_per_block,dec_temps,rew, utility, conf]\n",
    "\n",
    "pooled(arrays)\n",
    "names = load_file_names(arrays)\n",
    "print(names)\n",
    "# df = load_df(names, data_folder='temp/old',extinguish=extinguish)\n",
    "df = load_df(names, data_folder='temp',extinguish=extinguish)\n",
    "df_dkl = load_df_reward_dkl(names, data_folder='temp')\n",
    "df.head()\n",
    "\n",
    "\n",
    "switch = False\n",
    "contingency_degr = True\n",
    "reward_naive = True\n",
    "q = context_trans_prob[0]\n",
    "h = 200\n",
    "trials_per_block = 70\n",
    "training_blocks = 4\n",
    "degradation_blocks =  6\n",
    "p = cue_ambiguity[0]\n",
    "dec_temp = 1\n",
    "one_run = True\n",
    "util = utility[-1]\n",
    "util = [u/100 for u in util]\n",
    "\n",
    "rew = rews[0]\n",
    "# strs = np.array(['switch_cues==', '& contingency_degradation==', '& learn_rew==', '& q==', '& h<=',\\\n",
    "#                 '& training_blocks==', '& degradation_blocks<=10', '& trials_per_block==', '& dec_temp =='],dtype='str')\n",
    "\n",
    "# vals = np.array([switch, contingency_degr, reward_naive, q, h,\\\n",
    "#                 training_blocks, degradation_blocks, trials_per_block, dec_temp], dtype='str')\n",
    "\n",
    "strs = np.array(['switch_cues==', '& contingency_degradation==', '& learn_rew==', '& q==', '& h<=', '& p==', \\\n",
    "                '& training_blocks==', '& degradation_blocks==', '& trials_per_block==', '& dec_temp ==',\\\n",
    "                '& utility_0==', '& utility_1==', '& utility_2==', '& r_lambda=='],dtype='str')\n",
    "vals = np.array([switch, contingency_degr, reward_naive, q, h, p,\\\n",
    "                training_blocks, degradation_blocks, trials_per_block, dec_temp, util[0], util[1], util[2],rew], dtype='str')\n",
    "whole_query = np.char.join('', np.char.add(strs, vals))\n",
    "base_query = ' '.join(whole_query.tolist())\n",
    "\n",
    "base_df_dkl = df_dkl.query(base_query)\n",
    "print(base_df_dkl.size)\n",
    "print(whole_query)\n",
    "for di, db in enumerate([6]):\n",
    "    # print(db)\n",
    "    plot_df = base_df_dkl.query('t==3' + ' & degradation_blocks ==' + str(db))\n",
    "    print(plot_df.shape)\n",
    "    fig, axes = plt.subplots(ncols=4, nrows=1, figsize=(20,3))\n",
    "    for cont in range(4):\n",
    "\n",
    "        # print('context_cues == ' + str(cue) + ' & context== ' + str(cont))\n",
    "        if one_run == True:\n",
    "            quer = 'context== ' + str(cont) + ' & run == 0'\n",
    "        else:\n",
    "            quer = 'context== ' + str(cont)\n",
    "        if cont == 3:\n",
    "            sns.lineplot(ax=axes[cont], data=plot_df.query(quer),\n",
    "                        x='trial', y='avg_dkl', hue='h', legend=True, \\\n",
    "                        palette=sns.color_palette('Reds_r',n_colors=np.unique(plot_df['h']).size))\n",
    "            # print(np.unique(plot_df['h']))\n",
    "            axes[cont].legend(ncol = np.unique(plot_df['h']).size, bbox_to_anchor=(-3.3, -0.3), loc='upper left',\\\n",
    "                borderaxespad=0,title='h')\n",
    "        else:\n",
    "            \n",
    "            sns.lineplot(ax=axes[cont], data=plot_df.query(quer),\n",
    "                        x='trial', y='avg_dkl', hue='h',legend=False,\\\n",
    "                            palette=sns.color_palette('Reds_r',n_colors=np.unique(plot_df['h']).size))\n",
    "        axes[cont].set_title('context: ' + str(cont))\n",
    "\n",
    "    ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "    cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "    for ax in axes.flatten():    \n",
    "        for i, row in ranges.iterrows():\n",
    "            ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "\n",
    "plt.figure()\n",
    "# sns.histplot(data=plot_df.query('t==0 & trial_type == 1 & cue = 0', x ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimality only during extinction for different context ambiguities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation plot for context beliefs\n",
    "\n",
    "\n",
    "Top panel shows posterior over contexts at t==3;</br>\n",
    "bottom 3 panels shows policy, prior, likelihood and posterior;</br>\n",
    "</br>\n",
    "Legend is shit because idk how to control it but basically:\n",
    "</br>\n",
    "</br>\n",
    "top panel: blue = h1, red = h100</br>\n",
    "bottom panels: blue solid - h1, training context;</br>\n",
    "               blue solid - h1, extinction context;</br>\n",
    "               red solid - h100, training context;</br>\n",
    "               red solid - h100, extinction context;</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Load actual dataframe\n",
    "\n",
    "# nc = 4\n",
    "# extinguish = True\n",
    "# h =  [1,100]\n",
    "# cue_ambiguity = [0.8]                       \n",
    "# context_trans_prob = [1/nc]                \n",
    "# degradation = [True]\n",
    "# cue_switch = [False]\n",
    "# reward_naive = [False]\n",
    "# training_blocks = [4]\n",
    "# degradation_blocks=[4]\n",
    "# trials_per_block=[70]\n",
    "# arrays = [cue_switch, degradation, reward_naive, context_trans_prob, cue_ambiguity,h,\\\n",
    "#         training_blocks, degradation_blocks, trials_per_block]\n",
    "\n",
    "# names = load_file_names(arrays)\n",
    "# df_context = load_df_animation_context(names)\n",
    "# df_policy = load_df_animation_pol(names)\n",
    "# print(np.unique(df_policy['h']).size)\n",
    "# df_policy = df_policy.query('run == 0')\n",
    "# cntxts = ['(context == 0 | context == 2)', '(context == 1 | context == 3)']\n",
    "\n",
    "\n",
    "\n",
    "# ### prepare figure plotting \n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# def animation_figure():\n",
    "#     ##### figure 1\n",
    "#     fig1 = px.line(plot_df, x='policy', y='policy_prior',animation_frame='trial',animation_group='context',\\\n",
    "#         color=\"h\",line_dash = 'context', range_y=[0,1])\n",
    "#     fig2 = px.line(plot_df, x='policy', y='policy_like',animation_frame='trial',animation_group='context',\\\n",
    "#         color=\"h\",line_dash = 'context', range_y=[0,1])\n",
    "#     fig3 = px.line(plot_df, x='policy', y='policy_post',animation_frame='trial',animation_group='context',\\\n",
    "#         color=\"h\",line_dash = 'context', range_y=[0,1])\n",
    "#     fig4 = px.line(base_df.query(trial_que + ' & t==1' + ' & policy==0'),\\\n",
    "#         x='context', y='post_context',animation_frame='trial',animation_group='h', color=\"h\", range_y=[0,1])\n",
    "#     fig4\n",
    "\n",
    "#     # ##### combine\n",
    "\n",
    "#     # integrate the two figures, putting second figure on separate axis\n",
    "\n",
    "#     fig = go.Figure(\n",
    "#         data=[t for t in fig1.data] \n",
    "#             + [t.update(xaxis=\"x2\", yaxis=\"y2\") for t in fig2.data] \n",
    "#             + [t.update(xaxis=\"x3\", yaxis=\"y3\") for t in fig3.data] \n",
    "#             + [t.update(xaxis=\"x4\", yaxis=\"y4\") for t in fig4.data],\n",
    "#         frames=[\n",
    "#             go.Frame(\n",
    "#                 name=fr1.name,\n",
    "#                 data=[t for t in fr1.data]\n",
    "#                 + [t.update(xaxis=\"x2\", yaxis=\"y2\") for t in fr2.data]\n",
    "#                 + [t.update(xaxis=\"x3\", yaxis=\"y3\") for t in fr3.data]\n",
    "#                 + [t.update(xaxis=\"x4\", yaxis=\"y4\") for t in fr4.data],\n",
    "#             )\n",
    "#             for fr4, fr3, fr2, fr1 in zip(fig4.frames, fig3.frames, fig2.frames, fig1.frames)\n",
    "#         ],\n",
    "#         layout=fig1.layout,\n",
    "#     )\n",
    "\n",
    "\n",
    "#     # now config axes appropriately\n",
    "#     fig.update_layout(\n",
    "#         xaxis_domain=[0, 0.3],\n",
    "#         yaxis_domain=[0, 0.45],\n",
    "#         yaxis_range=[0, 1],\n",
    "\n",
    "#         xaxis2={\"domain\":[0.33, 0.63], \"matches\": None, \"title\":{\"text\":fig2.layout.xaxis.title.text}},\n",
    "#         yaxis2={\"range\": [0,1], \"domain\":[0, 0.45],'position':0.33, \"matches\":None,\"title\":{\"text\":fig2.layout.yaxis.title.text}},\n",
    "\n",
    "#         xaxis3={\"domain\": [0.66, 1], \"matches\": None, \"title\":{\"text\":fig3.layout.xaxis.title.text}},\n",
    "#         yaxis3={\"range\": [0,1],\"domain\":[0, 0.45], \"matches\": None,'position':0.66, \"title\":{\"text\":fig3.layout.yaxis.title.text}},\n",
    "#         xaxis4={\"domain\": [0, 0.5], \"matches\": None,\"showticklabels\":False},\n",
    "#         yaxis4={\"range\": [0,1], \"domain\":[0.5, 1], \"matches\": None, \"title\":{\"text\":fig4.layout.yaxis.title.text}},\n",
    "\n",
    "#         showlegend=True,\n",
    "#     )\n",
    "\n",
    "#     for i, frame in enumerate(fig.frames):\n",
    "#         frame.layout.title = \"Cue: {}, Trial type: {}, True optimal:{}\".format(cue, trial_type[i],true_optimal[i])\n",
    "#     for step in fig.layout.sliders[0].steps:\n",
    "#         step[\"args\"][1][\"frame\"][\"redraw\"] = True\n",
    "\n",
    "#     fig.show()\n",
    "\n",
    "\n",
    "# ### select parameters for figure plotting\n",
    "# ### cue is most important here and you can change it to get an idea for different cues\n",
    "\n",
    "\n",
    "# cue = 1\n",
    "# switch = False\n",
    "# contingency_degr = degradation[0]\n",
    "# reward_naive = reward_naive[0]\n",
    "# q = 0.25\n",
    "# trials_per_block = 70\n",
    "# training_blocks = training_blocks[0]\n",
    "# degradation_blocks = degradation_blocks[0]\n",
    "# p = cue_ambiguity[0]\n",
    "\n",
    "# strs = np.array(['switch_cues == ', '& contingency_degradation==', '& learn_rew==', '& q==', '& h>=',\\\n",
    "#                  '& training_blocks == ', '& degradation_blocks == ', '& trials_per_block ==', '& p =='],dtype='str')\n",
    "# vals = np.array([switch, contingency_degr, reward_naive, q, 1,\\\n",
    "#                  training_blocks, degradation_blocks, trials_per_block, p], dtype='str')\n",
    "# whole_query = np.char.join('', np.char.add(strs, vals))\n",
    "# base_query = ' '.join(whole_query.tolist())\n",
    "# base_df = df_policy.query(base_query)\n",
    "# trial_type = plot_df.groupby(by=['trial'])['trial_type'].mean().to_numpy().astype('int32')\n",
    "# true_optimal = plot_df.groupby(by=['trial'])['true_optimal'].mean().to_numpy().astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "# ### here you can change which trial type you want to look at\n",
    "\n",
    "# trial_que = 'trial_type == 0  & context_cues ==' + str(cue)     # training\n",
    "# trial_que = 'trial_type == 1  & context_cues ==' + str(cue)   # degradation\n",
    "# # trial_que = 'trial_type == 2  & context_cues ==' + str(cue)   # extinction\n",
    "\n",
    "# plot_df = base_df.query(trial_que + ' & t==0' + ' &' + cntxts[cue])\n",
    "# trial_type = plot_df.groupby(by=['trial'])['trial_type'].mean().to_numpy().astype('int32')\n",
    "# true_optimal = plot_df.groupby(by=['trial'])['true_optimal'].mean().to_numpy().astype('int32')\n",
    "# animation_figure()\n",
    "\n",
    "\n",
    "# trial_que = 'trial_type == 1  & context_cues ==' + str(cue)   # degradation\n",
    "# plot_df = base_df.query(trial_que + ' & t==0' + ' &' + cntxts[cue])\n",
    "# trial_type = plot_df.groupby(by=['trial'])['trial_type'].mean().to_numpy().astype('int32')\n",
    "# true_optimal = plot_df.groupby(by=['trial'])['true_optimal'].mean().to_numpy().astype('int32')\n",
    "# animation_figure()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
