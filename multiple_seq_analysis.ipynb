{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run first\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import chart_studio.plotly as py\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cufflinks as cf\n",
    "import json as js\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "import itertools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=False)\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook_connected'\n",
    "import os\n",
    "import action_selection as asl\n",
    "from itertools import product, repeat\n",
    "import jsonpickle as pickle\n",
    "import jsonpickle.ext.numpy as jsonpickle_numpy\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "import perception as prc\n",
    "import agent as agt\n",
    "from environment import PlanetWorld\n",
    "from agent import BayesianPlanner\n",
    "from world import World\n",
    "from planet_sequences import generate_trials_df\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "# functions\n",
    "def load_file_names(arrays, use_fitting=False):\n",
    "    lst = []\n",
    "    for i in product(*arrays):\n",
    "        lst.append(list(i))\n",
    "    \n",
    "    names = []\n",
    "    print('files to load: ' + str(len(lst)))\n",
    "    for li, l in enumerate(lst):\n",
    "\n",
    "        prefix = 'multiple_'\n",
    "        if use_fitting == True:\n",
    "            prefix += 'fitt_'\n",
    "        else:\n",
    "            prefix +='hier_'\n",
    "\n",
    "        if l[0] == True:\n",
    "            prefix += 'switch1_'\n",
    "        else:\n",
    "            prefix +='switch0_'\n",
    "\n",
    "        if l[1] == True:\n",
    "            prefix += 'degr1_'\n",
    "        else:\n",
    "            prefix += 'degr0_'\n",
    "\n",
    "        fname = prefix + 'p' + str(l[4])  +'_learn_rew' + str(int(l[2] == True))+ '_q' + str(l[3]) + '_h' + str(l[5]) + '_' +\\\n",
    "        str(l[8]) + '_' + str(l[6]) + str(l[7])+ '_dec' + str(l[9])\n",
    "        # print(len(l))\n",
    "        if len(l) > 10:\n",
    "            fname += '_' + l[-1]\n",
    "\n",
    "        fname +=  '_extinguish.json'\n",
    "\n",
    "        names.append(fname)\n",
    "\n",
    "\n",
    "    return names\n",
    "def load_df(names,data_folder='data', extinguish=None):\n",
    "    if extinguish is None:\n",
    "        raise('did not specify if rewarded during extinction')\n",
    "    # if not just_simulated:\n",
    "    path = os.path.join(os.getcwd(),data_folder)\n",
    "    #     names = os.listdir(path)\n",
    "    for fi, f in enumerate(names):\n",
    "        names[fi] = os.path.join(path,f)\n",
    "        # print(names[fi])\n",
    "\n",
    "    dfs = [None]*len(names)\n",
    "\n",
    "    for f,fname in enumerate(names):\n",
    "        jsonpickle_numpy.register_handlers()\n",
    "        with open(fname, 'r') as infile:\n",
    "            data = json.load(infile)\n",
    "        worlds = pickle.decode(data)\n",
    "        meta = worlds[-1]\n",
    "        agents = [w.agent for w in worlds[:-1]]\n",
    "        perception = [w.agent.perception for w in worlds[:-1]]\n",
    "        nt = worlds[0].T\n",
    "        npl = perception[0].npl\n",
    "        nr = worlds[0].agent.nr\n",
    "        nc = perception[0].nc\n",
    "        nw = len(worlds[:-1])\n",
    "        ntrials = meta['trials']\n",
    "        learn_rew = np.repeat(meta['learn_rew'], ntrials*nw*nt)\n",
    "        switch_cues = np.repeat(meta['switch_cues'], ntrials*nw*nt)\n",
    "        contingency_degradation = np.repeat(meta['contingency_degradation'], ntrials*nw*nt)\n",
    "        tr_per_block = np.repeat(meta['trials_per_block'], ntrials*nw*nt)\n",
    "        ndb = np.repeat(meta['degradation_blocks'], ntrials*nw*nt)\n",
    "        ntb = np.repeat(meta['training_blocks'], ntrials*nw*nt)\n",
    "        post_dir_rewards = [a.posterior_dirichlet_rew for a in agents]\n",
    "        post_dir_rewards = [post[:,1:,:,:] for post in post_dir_rewards]\n",
    "        entropy_rewards = np.zeros([nw*ntrials*nt,nc])\n",
    "        extinguished = np.zeros(ntrials*nw*nt, dtype='int32')\n",
    "        extinguished[:] = int(extinguish == True)\n",
    "        \n",
    "        for ip, post in enumerate(post_dir_rewards):\n",
    "            post = post.sum(axis=3)                      # sum out planets\n",
    "            norm = post.sum(axis=2)                      # normalizing constants\n",
    "            reward_distributions = np.zeros(post.shape)\n",
    "\n",
    "            for r in range(nr):                          # normalize each reward \n",
    "                reward_distributions[:,:,r,:] = np.divide(post[:,:,r,:],norm)\n",
    "            entropy = np.zeros([ntrials, nt, nc])\n",
    "\n",
    "            for trl in range(ntrials):\n",
    "                for t in range(nt-1):\n",
    "                    prob = reward_distributions[trl,t,:,:].T\n",
    "                    # if prob.sum() == 0:\n",
    "                    #     print('problem')\n",
    "                    entropy[trl,t+1,:] = -(np.log(prob)*prob).sum(axis=1)\n",
    "\n",
    "            entropy[:,0,:] = None\n",
    "            entropy_rewards[ip*(ntrials*nt):(ip+1)*(ntrials*nt),:] = np.reshape(entropy, [ntrials*nt,nc])\n",
    "\n",
    "        entropy_context = np.zeros(ntrials*nt*nw)\n",
    "        post_context = [a.posterior_context for a in agents]\n",
    "\n",
    "        for ip, post in enumerate(post_context):\n",
    "            entropy = np.zeros([ntrials, nt])\n",
    "\n",
    "            for trl in range(ntrials):\n",
    "                entropy[trl,:] = -(np.log(post[trl,:])*post[trl,:]).sum(axis=1) \n",
    "            entropy_context[ip*(ntrials*nt):(ip+1)*(ntrials*nt)] = np.reshape(entropy, [ntrials*nt])\n",
    "\n",
    "        posterior_context = [agent.posterior_context for agent in agents]\n",
    "        observations = [w.observations for w in worlds[:-1]]\n",
    "        context_cues = worlds[0].environment.context_cues\n",
    "        policies = worlds[0].agent.policies\n",
    "        actions = [w.actions[:,:3] for w in worlds[:-1]] \n",
    "        true_optimal = np.tile(np.repeat(meta['optimal_sequence'],nt), nw)\n",
    "        cue = np.tile(np.repeat(context_cues, nt), nw)\n",
    "        ex_p = np.zeros(ntrials)\n",
    "        executed_policy = np.zeros(nw*ntrials,dtype='int32')\n",
    "        optimality = np.zeros(nw*ntrials)\n",
    "        chose_optimal = np.zeros(nw*ntrials)\n",
    "\n",
    "        for w in range(nw):\n",
    "            for pi, p in enumerate(policies):\n",
    "                inds = np.where( (actions[w][:,0] == p[0]) & (actions[w][:,1] == p[1]) & (actions[w][:,2] == p[2]) )[0]\n",
    "                ex_p[inds] = pi\n",
    "            executed_policy[w*ntrials:(w+1)*ntrials] = ex_p\n",
    "            ch_op = executed_policy[w*ntrials:(w+1)*ntrials] == meta['optimal_sequence']\n",
    "            chose_optimal[w*ntrials:(w+1)*ntrials] = ch_op\n",
    "            optimality[w*ntrials:(w+1)*ntrials] = np.cumsum(ch_op)/(np.arange(ntrials)+1)\n",
    "\n",
    "        executed_policy = np.repeat(executed_policy, nt)\n",
    "        chose_optimal = np.repeat(chose_optimal, nt)\n",
    "        optimality = np.repeat(optimality, nt)\n",
    "        no = perception[0].generative_model_context.shape[0]\n",
    "        optimal_contexts = [np.argmax(perception[0].generative_model_contexts[i,:] for i in range(no))]\n",
    "        true_context = 0\n",
    "        q = np.repeat(meta['context_trans_prob'], ntrials*nw*nt)\n",
    "        p = np.repeat(meta['cue_ambiguity'], ntrials*nw*nt)\n",
    "        h = np.repeat(meta['h'], ntrials*nw*nt)\n",
    "        dec_temp = np.repeat(worlds[0].dec_temp,ntrials*nw*nt)\n",
    "        switch_cues = np.repeat(meta['switch_cues'], ntrials*nw*nt)\n",
    "        learn_rew = np.repeat(meta['learn_rew'], ntrials*nw*nt)\n",
    "        degradation = np.repeat('contingency_degradation', ntrials*nw*nt)\n",
    "        trial_type = np.tile(np.repeat(meta['trial_type'], nt), nw)\n",
    "        trial = np.tile(np.repeat(np.arange(ntrials),nt), nw)\n",
    "        run = np.repeat(np.arange(nw),nt*ntrials)\n",
    "        run.astype('str')\n",
    "        inferred_context_t0 = np.zeros(ntrials*nw,dtype='int32')\n",
    "        inferred_context_t3  = np.zeros(ntrials*nw,'int32')\n",
    "        agnt = np.repeat(np.arange(nw)+f*nw,nt*ntrials)\n",
    "        true_context = np.zeros(ntrials*nw, dtype='int32')\n",
    "        no, nc = perception[0].generative_model_context.shape \n",
    "        modes_gmc =  perception[0].generative_model_context.argsort(axis=1)\n",
    "        contexts = [modes_gmc[i,:][-2:] for i in range(no)] # arranged in ascending order!\n",
    "        if_inferred_context_switch = np.zeros(ntrials, dtype=\"int32\")\n",
    "\n",
    "        # c is an array holding what contexts should be for the given task trials [0,1,0,1,..2,3,2,3...0,1] etc\n",
    "        to = np.zeros(ntrials, dtype=\"int32\")\n",
    "        for i in range(no):    \n",
    "            c = np.array([contexts[i][-1]]*(meta['trials_per_block']*meta['training_blocks'])\\\n",
    "                + [contexts[i][-2]]*(meta['trials_per_block']*meta['degradation_blocks'])\\\n",
    "                + [contexts[i][-1]]*meta['trials_per_block']*2)\n",
    "            to[np.where(context_cues == i)] = c[np.where(context_cues == i)]\n",
    "            # print(to)\n",
    "            if_inferred_context_switch[np.where(context_cues == i)] = c[np.where(context_cues == i)]\n",
    "        inferred_switch = np.zeros(ntrials*nw,dtype='int32')\n",
    "        context_optimality = np.zeros(ntrials*nw)\n",
    "\n",
    "        for w in range(nw):\n",
    "            inferred_context_t0[w*ntrials:(w+1)*ntrials] = np.argmax(posterior_context[w][:,0,:],axis=1)\n",
    "            inferred_context_t3[w*ntrials:(w+1)*ntrials] = np.argmax(posterior_context[w][:,-1,:],axis=1)\n",
    "            inferred_switch[w*ntrials:(w+1)*ntrials] = if_inferred_context_switch == \\\n",
    "                                                    inferred_context_t3[w*ntrials:(w+1)*ntrials]\n",
    "            context_optimality[w*ntrials:(w+1)*ntrials] = np.cumsum(inferred_switch[w*ntrials:(w+1)*ntrials])\\\n",
    "                                                                /(np.arange(ntrials)+1)\n",
    "        true_context[w*ntrials:(w+1)*ntrials] = to\n",
    "        inferred_switch = np.repeat(inferred_switch, nt)\n",
    "        inferred_context_t0 = np.repeat(inferred_context_t0, nt)\n",
    "        inferred_context_t3 = np.repeat(inferred_context_t3, nt)\n",
    "        context_optimality = np.repeat(context_optimality, nt)\n",
    "        true_context =  np.repeat(true_context, nt)\n",
    "        t = np.tile(np.arange(4), nw*ntrials)\n",
    "        \n",
    "        # print(true_context.size)\n",
    "        # print(trial_type.size)\n",
    "        d = {'trial_type':trial_type, 'run':run, 'trial':trial, 't':t, 'true_optimal':true_optimal,\\\n",
    "                            'cue':cue, 'q':q, 'p':p, 'h':h, 'inferred_context_t0':inferred_context_t0,\\\n",
    "                            'inferred_context_t3':inferred_context_t3, 'executed_policy':executed_policy,\\\n",
    "                            'chose_optimal': chose_optimal, 'entropy_rew_c1': entropy_rewards[:,0], 'entropy_rew_c2': entropy_rewards[:,1], \\\n",
    "                            'entropy_rew_c3': entropy_rewards[:,2] , 'entropy_rew_c4': entropy_rewards[:,3],\\\n",
    "                            'policy_optimality':optimality,'agent':agnt, 'inferred_switch': inferred_switch,\\\n",
    "                            'context_optimality':context_optimality, 'learn_rew': learn_rew, 'entropy_context':entropy_context, \\\n",
    "                            'switch_cues':switch_cues, 'contingency_degradation': contingency_degradation,\\\n",
    "                            'degradation_blocks': ndb, 'training_blocks':ntb, 'trials_per_block': tr_per_block,\\\n",
    "                            'true_context': true_context, 'dec_temp':dec_temp} \n",
    "\n",
    "        # for key in d.keys():\n",
    "        #     print(key, np.unique(d[key].shape))\n",
    "        dfs[f] = pd.DataFrame(d)\n",
    "        \n",
    "    data = pd.concat(dfs)\n",
    "\n",
    "    groups = ['agent','run', 't','degradation_blocks', 'training_blocks', 'trials_per_block','learn_rew', 'p', 'q','h','cue']\n",
    "    grouped = data.groupby(by=groups)\n",
    "    data['iterator'] = 1\n",
    "    data['ith_cue_trial'] = grouped['iterator'].transform('cumsum')\n",
    "    data['policy_optimality_cue'] = grouped['chose_optimal'].transform('cumsum') / data['ith_cue_trial']\n",
    "    data['context_optimal_cue'] = grouped['inferred_switch'].transform('cumsum') / data['ith_cue_trial']\n",
    "    data.drop('iterator',1,inplace =True)\n",
    "    data.astype({'h': 'category'})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def context_plot(query='p == 0.6'):\n",
    "\n",
    "    # context and policy optimality\n",
    "    fig = plt.figure(figsize=(10,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    print('cue')\n",
    "    plot_df = base_df.query(query + '& t ==' + str(t) + ' & cue == ' + str(cue))\n",
    "    plot_df['h'] = plot_df['h'].astype('category')\n",
    "    ax = sns.lineplot(data=plot_df, x='trial', y='context_optimality', hue='h',\\\n",
    "                      palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size), legend=False)\n",
    "    ax.set(ylim = (0,1.1))\n",
    "    ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "    cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "    for i, row in ranges.iterrows():\n",
    "        ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "    # reward distribution entropy for different contexts\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = sns.lineplot(data=plot_df, x='trial', y='policy_optimality', hue='h',\\\n",
    "            palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size))\n",
    "    ax.legend(ncol = np.unique(plot_df['h']).size, bbox_to_anchor=(-2, -0.25), loc='upper left',\\\n",
    "              borderaxespad=0,title='h')\n",
    "    cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "    for i, row in ranges.iterrows():\n",
    "        ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "\n",
    "    ax.set(ylim = (0,1))\n",
    "    # title = base_query + ' & ' + query\n",
    "    # title = title.replace(' & ', ', ')\n",
    "    # title = title.replace('==', ':')\n",
    "    title = 'Inferred context and policy optimality for p: ' + query[-3:] + ', switch cues: '\\\n",
    "            + str(int(switch)) + ', degradation: ' + str(int(contingency_degr)) + \\\n",
    "            ', reward_naive: ' + str(int(reward_naive)) + ', cue_shown: ' + str(cue)\n",
    "    fig.suptitle(title, fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "def reward_entropy_plot(query='p == 0.6'):\n",
    "\n",
    "    # entropy of reward distribution for each context \n",
    "    plot_df = base_df.query(query + '& t ==' + str(t) + ' & cue == ' + str(cue))\n",
    "    plot_df['h'] = plot_df['h'].astype('category')\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(11, 7))\n",
    "    axs_flat = axs.flatten()\n",
    "    ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "\n",
    "    legs = [False, False, False, True]\n",
    "    ys = ['entropy_rew_c1', 'entropy_rew_c2','entropy_rew_c3','entropy_rew_c4',]\n",
    "    for c in range(nc):\n",
    "        sns.lineplot(ax=axs_flat[c], data=plot_df, x='trial',y=ys[c],hue='h',\\\n",
    "            palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size), legend=legs[c])\n",
    "        cols = [[1,1,1], [0,0,0],[1,1,1]]\n",
    "        for i, row in ranges.iterrows():\n",
    "            axs_flat[c].axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "        axs_flat[-1].legend(ncol = np.unique(plot_df['h']).size, bbox_to_anchor=(-2, -0.25), loc='upper left',\\\n",
    "                            borderaxespad=0.5,title='h')\n",
    "    \n",
    "    title = 'Reward distribution entropy for  p: ' + query[-3:] + ', switch cues: ' + str(int(switch)) +\\\n",
    "            ', degradation: ' + str(int(contingency_degr)) + ', reward_naive: ' + str(int(reward_naive)) + \\\n",
    "            ', cue_shown: ' + str(cue)\n",
    "    fig.suptitle(title, fontsize=15)\n",
    "\n",
    "def context_plot_cue_dependent(query='p == 0.6',print_counts=False):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,6), sharex = True)\n",
    "\n",
    "    lgnd = [False, True]\n",
    "    cues = [0,1]\n",
    "    titles_c = ['Inferred context optimality for cue 0','Inferred context optimality (at t4) for cue 1']\n",
    "    titles_p = ['Inferred policy optimality for cue 0','Inferred policy optimality for cue 1']\n",
    "\n",
    "    for cue in cues:\n",
    "        plot_df = base_df.query(query + '& t ==' + str(t) + ' & cue == ' + str(cue))\n",
    "\n",
    "        sns.lineplot(ax = axes[0,cue], data=plot_df, x='trial', y='context_optimal_cue', hue='h',\\\n",
    "                      palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size),legend=False)\n",
    "        axes[0,cue].set_title(titles_c[cue])\n",
    "        axes[1,cue].set_title(titles_p[cue])\n",
    "        sns.lineplot(ax=axes[1,cue], data=plot_df, x='trial', y='policy_optimality_cue', hue='h',\\\n",
    "            palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size), legend=lgnd[cue])\n",
    "    axes[1,1].legend(ncol = np.unique(plot_df['h']).size, bbox_to_anchor=(-2, -0.25), loc='upper left',\\\n",
    "              borderaxespad=0,title='h')\n",
    "\n",
    "    if print_counts:\n",
    "        test_df =  base_df.query(query + '& t ==' + str(t))\n",
    "        test_df['correct'] = test_df['inferred_context_t3']== test_df['true_context']\n",
    "        cols = ['agent','h', 'trial','trial_type', 'cue', 'inferred_context_t3', 'true_context','correct']\n",
    "        counts = test_df[cols].groupby(by=['h','trial_type','cue','correct']).size()\n",
    "        print(counts)\n",
    "\n",
    "    ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "    cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "    for ax in axes.flatten():\n",
    "        for i, row in ranges.iterrows():\n",
    "            ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "    \n",
    "\n",
    "\n",
    "    # title = base_query + ' & ' + query\n",
    "    # title = title.replace(' & ', ', ')\n",
    "    # title = title.replace('==', ':')\n",
    "    title = 'p: ' + query[-3:] + ', switch cues: '\\\n",
    "            + str(int(switch)) + ', degradation: ' + str(int(contingency_degr)) + \\\n",
    "            ', reward_naive: ' + str(int(reward_naive))\n",
    "    fig.suptitle(title, fontsize=15)\n",
    "\n",
    "\n",
    "def load_df_animation_context(names,data_folder='temp'):\n",
    "\n",
    "    path = os.path.join(os.getcwd(),data_folder)\n",
    "    for fi, f in enumerate(names):\n",
    "        names[fi] = os.path.join(path,f)\n",
    "\n",
    "\n",
    "    overall_df = [None for _ in range(len(names))]\n",
    "    for f,fname in enumerate(names):\n",
    "        jsonpickle_numpy.register_handlers()\n",
    "\n",
    "        with open(fname, 'r') as infile:\n",
    "            data = json.load(infile)\n",
    "\n",
    "        worlds = pickle.decode(data)\n",
    "        meta = worlds[-1]\n",
    "        nw = len(worlds[:-1])\n",
    "        agents = [w.agent for w in worlds[:-1]]\n",
    "        posterior_context = [agent.posterior_context for agent in agents]\n",
    "        ntrials, t, nc = posterior_context[0].shape\n",
    "        outcome_surprise = [agent.outcome_suprise for agent in agents]\n",
    "        # context_obs_surprise = [agent.context_obs_surprise for agent in agents]\n",
    "        policy_surprise = [agent.policy_surprise for agent in agents]\n",
    "        policy_entropy = [agent.policy_entropy for agent in agents]\n",
    "        npi = agents[0].posterior_policies.shape[2]\n",
    "        taus = np.arange(ntrials)\n",
    "        ts = np.arange(t)\n",
    "        cs = np.arange(nc)\n",
    "        pis_post = np.array(['post_' for _ in range(npi)], dtype=object) + np.array([str(i) for i in range(npi)], dtype=object)\n",
    "        pis_prior = np.array(['post_' for _ in range(npi)], dtype=object) + np.array([str(i) for i in range(npi)], dtype=object)\n",
    "        pis_like = np.array(['post_' for _ in range(npi)], dtype=object) + np.array([str(i) for i in range(npi)], dtype=object)\n",
    "\n",
    "        mi = pd.MultiIndex.from_product([taus, ts, cs], names=['trial', 't', 'context'])\n",
    "        mi_post = pd.MultiIndex.from_product([taus, ts, pis_post, cs], names=['trial', 't', 'policy','context'])\n",
    "        mi_like = pd.MultiIndex.from_product([taus, ts, pis_like, cs], names=['trial', 't', 'policy','context'])\n",
    "        mi_prior = pd.MultiIndex.from_product([taus, ts, pis_like, cs], names=['trial', 't', 'policy','context'])\n",
    "\n",
    "        dfs = [None for _ in range(nw)]\n",
    "        factor = ntrials*nc*t\n",
    "\n",
    "        for w in range(nw):\n",
    "\n",
    "            policy_post_df =  pd.Series(index=mi_post, data=agents[w].posterior_policies.flatten())\n",
    "            policy_post_df = policy_post_df.unstack(level='policy').reset_index()\n",
    "            # fix entropy calcs\n",
    "            policy_prob = policy_post_df.iloc[:,-8:].to_numpy()\n",
    "            policy_prob[policy_prob == 0] = 10**(-300)\n",
    "            entropy = -(policy_prob*np.log(policy_prob)).sum(axis=1)\n",
    "\n",
    "            prior_policies = np.tile(agents[w].prior_policies[:,np.newaxis,:,:], (1,t,1,1))\n",
    "            # print(np.all(prior_policies[:,0,:,:] == prior_policies[:,1,:,:] ))\n",
    "            policy_prior_df =  pd.Series(index=mi_prior, data=prior_policies.flatten())\n",
    "            policy_prior_df = policy_prior_df.unstack(level='policy').reset_index()\n",
    "\n",
    "            policy_like_df =  pd.Series(index=mi_like, data=agents[w].likelihood.flatten())\n",
    "            policy_like_df = policy_like_df.unstack(level='policy').reset_index()\n",
    "            \n",
    "            policy_entropy_df = pd.Series(index=mi,\\\n",
    "                data=policy_entropy[w].flatten()).reset_index().rename(columns = {0:'policy_entropy'})\n",
    "            \n",
    "            policy_surprise_df = pd.Series(index=mi,\\\n",
    "                data=policy_surprise[w].flatten()).reset_index().rename(columns = {0:'context_obs_surprise'})\n",
    "            outcome_surprise_df = pd.Series(index=mi,\\\n",
    "                data=outcome_surprise[w].flatten()).reset_index().rename(columns = {0:'outcome_surprise'})\n",
    "                \n",
    "            # break\n",
    "            df = pd.Series(index=mi, data=posterior_context[w].flatten())\n",
    "            df = df.reset_index().rename(columns = {0:'probability'})\n",
    "            df['context_obs_surprise'] = policy_surprise_df['context_obs_surprise']\n",
    "            df['outcome_surprise'] = outcome_surprise_df['outcome_surprise']\n",
    "            df['policy_entropy'] = policy_entropy_df['policy_entropy']\n",
    "            # df['policy_entropy'] = policy_entropy_df['policy_entropy']\n",
    "            df['learn_rew'] = np.repeat(meta['learn_rew'], factor)\n",
    "            df['switch_cues'] = np.repeat(meta['switch_cues'], factor)\n",
    "            df['contingency_degradation'] = np.repeat(meta['contingency_degradation'], factor)\n",
    "            df['trials_per_block'] = np.repeat(meta['trials_per_block'], factor)\n",
    "            df['degradation_blocks'] = np.repeat(meta['degradation_blocks'], factor)\n",
    "            df['training_blocks'] = np.repeat(meta['training_blocks'], factor)\n",
    "            df['context_cues'] = np.repeat(worlds[0].environment.context_cues, nc*t)\n",
    "            df['true_optimal'] = np.repeat(meta['optimal_sequence'], nc*t)\n",
    "            df['q'] = np.repeat(meta['context_trans_prob'], factor)\n",
    "            df['p'] = np.repeat(meta['cue_ambiguity'], factor)\n",
    "            df['h'] = np.repeat(meta['h'], factor)\n",
    "            df['run'] = np.repeat(w,factor)\n",
    "            df['trial_type'] = np.repeat(meta['trial_type'], nc*t)\n",
    "            df['trial'] = np.repeat(np.arange(ntrials), nc*t)\n",
    "            df['agent'] = np.repeat(w+f*nw,factor)\n",
    "            df = df.join(policy_post_df.iloc[:,-8:])\n",
    "            dfs[w] = df\n",
    "            break\n",
    "        overall_df[f] = pd.concat(dfs)\n",
    "    data_animation = pd.concat(overall_df)\n",
    "    # data_animation.to_excel('data_animation.xlsx')\n",
    "    return data_animation\n",
    "# data_animation.to_csv('data_animation.csv')\n",
    "\n",
    "\n",
    "def load_df_animation_pol(names,data_folder='temp'):\n",
    "\n",
    "    path = os.path.join(os.getcwd(),data_folder)\n",
    "    #     names = os.listdir(path)\n",
    "\n",
    "    for fi, f in enumerate(names):\n",
    "        names[fi] = os.path.join(path,f)\n",
    "\n",
    "\n",
    "    overall_df = [None for _ in range(len(names))]\n",
    "    for f,fname in enumerate(names):\n",
    "        jsonpickle_numpy.register_handlers()\n",
    "\n",
    "        with open(fname, 'r') as infile:\n",
    "            data = json.load(infile)\n",
    "\n",
    "        worlds = pickle.decode(data)\n",
    "        meta = worlds[-1]\n",
    "        nw = len(worlds[:-1])\n",
    "        agents = [w.agent for w in worlds[:-1]]\n",
    "        posterior_context = [agent.posterior_context for agent in agents]\n",
    "        ntrials, t, nc = posterior_context[0].shape\n",
    "        policy_entropy = [agent.policy_entropy for agent in agents]\n",
    "        npi = agents[0].posterior_policies.shape[2]\n",
    "        taus = np.arange(ntrials)\n",
    "        ts = np.arange(t)\n",
    "        cs = np.arange(nc)\n",
    "        pis = np.arange(npi)\n",
    "\n",
    "        mi = pd.MultiIndex.from_product([taus, ts, pis, cs], names=['trial', 't','policy', 'context'])\n",
    "        dfs = [None for _ in range(nw)]\n",
    "        factor = ntrials*nc*t*npi\n",
    "\n",
    "        for w in range(nw):\n",
    "            df =  pd.Series(index=mi, data=agents[w].posterior_policies.flatten())\n",
    "            df = df.reset_index().rename(columns = {0:'policy_post'})\n",
    "\n",
    "            prior_policies = np.tile(agents[w].prior_policies[:,np.newaxis,:,:], (1,t,1,1))\n",
    "            policy_prior_df =  pd.Series(index=mi, data=prior_policies.flatten())\n",
    "            policy_prior_df = policy_prior_df.reset_index().rename(columns = {0:'policy_prior'})\n",
    "\n",
    "            policy_like_df =  pd.Series(index=mi, data=agents[w].likelihood.flatten())\n",
    "            policy_like_df = policy_like_df.reset_index().rename(columns = {0:'policy_likelihood'})\n",
    "            df['policy_prior'] = policy_prior_df['policy_prior']\n",
    "            df['policy_like'] = policy_like_df['policy_likelihood']\n",
    "            post_context = np.tile(posterior_context[w][:,:,np.newaxis,:], (1,1,npi,1))\n",
    "            # print(np.all(post_context[:,:,0,:] == post_context[:,:,3,:]))\n",
    "\n",
    "            post_context_df = pd.Series(index=mi, data=post_context.flatten())\n",
    "            post_context_df = post_context_df.reset_index().rename(columns = {0:'post_context'})\n",
    "            df['post_context'] = post_context_df['post_context']\n",
    "            df['learn_rew'] = np.repeat(meta['learn_rew'], factor)\n",
    "            df['switch_cues'] = np.repeat(meta['switch_cues'], factor)\n",
    "            df['contingency_degradation'] = np.repeat(meta['contingency_degradation'], factor)\n",
    "            df['trials_per_block'] = np.repeat(meta['trials_per_block'], factor)\n",
    "            df['degradation_blocks'] = np.repeat(meta['degradation_blocks'], factor)\n",
    "            df['training_blocks'] = np.repeat(meta['training_blocks'], factor)\n",
    "            df['context_cues'] = np.repeat(worlds[0].environment.context_cues, nc*t*npi)\n",
    "            df['true_optimal'] = np.repeat(meta['optimal_sequence'], nc*t*npi)\n",
    "            df['q'] = np.repeat(meta['context_trans_prob'], factor)\n",
    "            df['p'] = np.repeat(meta['cue_ambiguity'], factor)\n",
    "            df['h'] = np.repeat(meta['h'], factor)\n",
    "            df['run'] = np.repeat(w,factor)\n",
    "            df['trial_type'] = np.repeat(meta['trial_type'], nc*t*npi)\n",
    "            df['trial'] = np.repeat(np.arange(ntrials), nc*t*npi)\n",
    "            df['agent'] = np.repeat(w+f*nw,factor)\n",
    "            dfs[w] = df\n",
    "\n",
    "        overall_df[f] = pd.concat(dfs)\n",
    "    data_animation = pd.concat(overall_df)\n",
    "    # data_animation.to_excel('data_animation.xlsx')\n",
    "    return data_animation\n",
    "\n",
    "\n",
    "def load_df_reward_dkl(names,data_folder='temp',nc=4):\n",
    "\n",
    "    path = os.path.join(os.getcwd(),data_folder)\n",
    "    #     names = os.listdir(path)\n",
    "\n",
    "    for fi, f in enumerate(names):\n",
    "        names[fi] = os.path.join(path,f)\n",
    "\n",
    "    dfs = [None]*len(names)\n",
    "\n",
    "    planet_reward_probs = np.array([[0.95, 0   , 0   ],\n",
    "                            [0.05, 0.95, 0.05],\n",
    "                            [0,    0.05, 0.95]])  \n",
    "\n",
    "    planet_reward_probs_switched = np.array([[0   , 0    , 0.95],\n",
    "                                            [0.05, 0.95 , 0.05],\n",
    "                                            [0.95, 0.05 , 0.0]])\n",
    "    \n",
    "    planet_reward_probs = np.tile(planet_reward_probs[:,:,np.newaxis], (1,1,nc))\n",
    "    planet_reward_probs_switched = np.tile(planet_reward_probs_switched[:,:,np.newaxis], (1,1,nc))\n",
    "\n",
    "    overall_df = [None for _ in range(len(names))]\n",
    "\n",
    "    for f,fname in enumerate(names):\n",
    "        jsonpickle_numpy.register_handlers()\n",
    "        with open(fname, 'r') as infile:\n",
    "            data = json.load(infile)\n",
    "                         \n",
    "        worlds = pickle.decode(data)\n",
    "        meta = worlds[-1]\n",
    "        agents = [w.agent for w in worlds[:-1]]\n",
    "        perception = [w.agent.perception for w in worlds[:-1]]\n",
    "        reward_probs = [agent.posterior_dirichlet_rew for agent in agents]\n",
    "        \n",
    "        nt = worlds[0].T\n",
    "        npl = perception[0].npl\n",
    "        nr = worlds[0].agent.nr\n",
    "        nc = perception[0].nc\n",
    "        nw = len(worlds[:-1])\n",
    "        ntrials = meta['trials']\n",
    "\n",
    "        learn_rew = np.repeat(meta['learn_rew'], ntrials*nw*nt)\n",
    "        switch_cues = np.repeat(meta['switch_cues'], ntrials*nw*nt)\n",
    "        contingency_degradation = np.repeat(meta['contingency_degradation'], ntrials*nw*nt)\n",
    "        ntrials_df = np.repeat(meta['trials_per_block'], ntrials*nw*nt)\n",
    "        ndb = np.repeat(meta['degradation_blocks'], ntrials*nw*nt)\n",
    "        ntb = np.repeat(meta['training_blocks'], ntrials*nw*nt)\n",
    "        post_dir_rewards = [a.posterior_dirichlet_rew for a in agents]\n",
    "        post_dir_rewards = [post[:,1:,:,:] for post in post_dir_rewards]\n",
    "        entropy_rewards = np.zeros([nw*ntrials*nt,nc])\n",
    "        extinguished = np.zeros(ntrials*nw*nt, dtype='int32')\n",
    "        extinguished[:] = int(extinguish == True)\n",
    "        \n",
    "        # define true distribution reward\n",
    "        if f == 0:\n",
    "            tpb = meta['trials_per_block']\n",
    "            db = meta['degradation_blocks']\n",
    "            tb = meta['training_blocks']\n",
    "            p= np.tile(planet_reward_probs[np.newaxis,np.newaxis,:,:,:], (ntrials, nt, 1,1,1))\n",
    "            p[tb*tpb:(tb+db)*tpb,:,:,:,:] = \\\n",
    "                np.tile(planet_reward_probs_switched[np.newaxis,np.newaxis,:,:,:],\n",
    "                       ((db + tb)*tpb - tb*tpb, nt, 1,1,1))\n",
    "            p[p == 0] = 10**(-300)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        factor = ntrials*nt*nc\n",
    "        taus = np.arange(ntrials)\n",
    "        ts = np.arange(nt)\n",
    "        # npls = np.char.add(np.asarray(['pl_' for _ in range(npl)]), np.asarray([str(i) for i in range(npl)]))\n",
    "        npls = np.arange(npl)\n",
    "        nrs = np.arange(nr)\n",
    "        cs = np.arange(nc)\n",
    "        mi = pd.MultiIndex.from_product([taus, ts, npls, cs],\n",
    "                names=['trial', 't', 'planet', 'context'])\n",
    "        # dfs_dkl = [None for _ in range(nw)]\n",
    "        # factor = ntrials*nt*nr*npl*nc\n",
    "\n",
    "        dkl_df = [None for _ in range(nw)]\n",
    "        for w in range(nw):\n",
    "            q = reward_probs[w]\n",
    "            e = (db+tb)*tpb\n",
    "            q[e:,:,:,:,:] = np.tile(q[e-1,:,:,:,:], (2*tpb,1,1,1,1))\n",
    "            q[q == 0] = 10**(-300)\n",
    "            norm = 1/(q.sum(axis=2))\n",
    "            q = np.einsum('etrpc, etpc -> etrpc', q, norm)\n",
    "            dkl = (q*np.log(q/p)).sum(axis=2)\n",
    "            df =  pd.Series(index=mi, data=dkl.flatten())\n",
    "            df = df.unstack(level = 'planet')\n",
    "            df = df.reset_index().rename(columns = {0:'p0_dkl', 1:'p1_dkl', 2:'p2_dkl'})\n",
    "            df['avg_dkl'] = (df['p0_dkl'] + df['p1_dkl'] + df['p2_dkl'])/3\n",
    "            df['learn_rew'] = np.repeat(meta['learn_rew'], factor)\n",
    "            df['switch_cues'] = np.repeat(meta['switch_cues'], factor)\n",
    "            df['contingency_degradation'] = np.repeat(meta['contingency_degradation'], factor)\n",
    "            df['trials_per_block'] = np.repeat(meta['trials_per_block'], factor)\n",
    "            df['degradation_blocks'] = np.repeat(meta['degradation_blocks'], factor)\n",
    "            df['training_blocks'] = np.repeat(meta['training_blocks'], factor)\n",
    "            df['context_cues'] = np.repeat(worlds[0].environment.context_cues, nc*nt)\n",
    "            df['true_optimal'] = np.repeat(meta['optimal_sequence'], nc*nt)\n",
    "            df['q'] = np.repeat(meta['context_trans_prob'], factor)\n",
    "            df['p'] = np.repeat(meta['cue_ambiguity'], factor)\n",
    "            df['h'] = np.repeat(meta['h'], factor)\n",
    "            df['run'] = np.repeat(w,factor)\n",
    "            df['trial_type'] = np.repeat(meta['trial_type'], nc*nt)\n",
    "            df['trial'] = np.repeat(np.arange(ntrials), nc*nt)\n",
    "            df['agent'] = np.repeat(w+f*nw,factor)\n",
    "            dkl_df[w] = df\n",
    "        #     break\n",
    "        # break\n",
    "        overall_df[f] = pd.concat(dkl_df)\n",
    "    data = pd.concat(overall_df)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# def context_entropy_plot(query='p == 0.6'):\n",
    "#     plot_df = base_df.query(query + '& t ==' + str(t) + ' & cue == ' + str(cue))\n",
    "#     plot_df['h'] = plot_df['h'].astype('category')\n",
    "#     fig,axes = plt.subplots(nrows=3, ncols=6,figsize = (27,15))\n",
    "#     hs = np.unique(plot_df['h'])\n",
    "#     ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "\n",
    "#     for ai, ax in enumerate(axes.flatten()[:-1]):\n",
    "#         plot_df = base_df.query(query + '& t ==' + str(t) + ' & cue == ' + str(cue) + ' & h==' + str(hs[ai]))\n",
    "#         sns.lineplot(ax=ax, data=plot_df, x='trial', y='entropy_context', legend=False)\n",
    "#         cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "#         for i, row in ranges.iterrows():\n",
    "#             ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "#         ax.set_title('h = ' + str(hs[ai]))\n",
    "#         ax.set(xlabel=None)\n",
    "#         ax.set(ylim=[0,0.85])\n",
    "#     ax.legend(ncol = np.unique(plot_df['h']).size, bbox_to_anchor=(-1, -0.25), loc='upper left',\\\n",
    "#                 borderaxespad=0,title='h')\n",
    "#     title = 'Entropy of posterior over context for  p: ' + query[-3:] + ', switch cues: ' +\\\n",
    "#                 str(int(switch)) + ', degradation: ' + str(int(contingency_degr)) +\\\n",
    "#                 ', reward_naive: ' + str(int(reward_naive)) + ', cue_shown: ' + str(cue)\n",
    "#     fig.suptitle(title, fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Load Data  <a class=\"anchor\" id=\"section_3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(os.getcwd(),'config/config_degradation_1_switch_0_train4_degr4_n70.json'))\n",
    "data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 4\n",
    "extinguish = True\n",
    "h =  [1, 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,30,40,50,60,70,80,90,100,200]\n",
    "# h=[1,40]\n",
    "# cue_ambiguity = [0.9, 0.8]                       \n",
    "# context_trans_prob = [0.9, 0.92, 0.93, 0.95, 0.97, 0.99]     \n",
    "# degradation = [True]\n",
    "# cue_switch = [False]\n",
    "# reward_naive = [False]\n",
    "# training_blocks = [4]\n",
    "# degradation_blocks=[4]\n",
    "# trials_per_block=[70]\n",
    "# dec_temp = [1]\n",
    "# conf = ['ordered']\n",
    "\n",
    "h = [1,10,40]\n",
    "cue_ambiguity = [0.9, 0.8]                       \n",
    "context_trans_prob = [0.9, 0.92, 0.93, 0.95, 0.97, 0.99]                \n",
    "degradation = [True]\n",
    "cue_switch = [False]\n",
    "reward_naive = [False]\n",
    "training_blocks = [2]\n",
    "degradation_blocks=[2]\n",
    "trials_per_block=[70]\n",
    "dec_temp = [1,4]\n",
    "conf = ['ordered']\n",
    "arrays = [cue_switch, degradation, reward_naive, context_trans_prob, cue_ambiguity,h,\\\n",
    "        training_blocks, degradation_blocks, trials_per_block,dec_temp,conf]\n",
    "\n",
    "\n",
    "names = load_file_names(arrays)\n",
    "print(names)\n",
    "# df = load_df(names, data_folder='temp/old',extinguish=extinguish)\n",
    "df = load_df(names, data_folder='temp',extinguish=extinguish)\n",
    "# df_dkl = load_df_reward_dkl(names, data_folder='temp')\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context optimality and choice optimality plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch = False\n",
    "contingency_degr = True\n",
    "reward_naive = False\n",
    "q = 0.9\n",
    "h = 40\n",
    "t = 3\n",
    "trials_per_block = 70\n",
    "training_blocks = 2\n",
    "degradation_blocks = 2\n",
    "queries = ['p==0.8']\n",
    "cue = 0\n",
    "dec_temp = 4\n",
    "one_run = True\n",
    "\n",
    "strs = np.array(['switch_cues==', '& contingency_degradation==', '& learn_rew==', '& q==', '& h<=',\\\n",
    "                 '& training_blocks==', '& degradation_blocks==', '& trials_per_block==', '& dec_temp =='],dtype='str')\n",
    "vals = np.array([switch, contingency_degr, reward_naive, q, h,\\\n",
    "                 training_blocks, degradation_blocks, trials_per_block, dec_temp], dtype='str')\n",
    "whole_query = np.char.join('', np.char.add(strs, vals))\n",
    "base_query = ' '.join(whole_query.tolist())\n",
    "\n",
    "if one_run == True:\n",
    "    base_query += ' & run == 0'\n",
    "\n",
    "base_df = df.query(base_query)\n",
    "# base_df_dkl = df_dkl.query(base_query)\n",
    "# queries = ['p==0.95','p==0.7']\n",
    "p = queries[0]\n",
    "context_plot_cue_dependent(p)\n",
    "\n",
    "\n",
    "dec_temp = 1\n",
    "strs = np.array(['switch_cues==', '& contingency_degradation==', '& learn_rew==', '& q==', '& h<=',\\\n",
    "                 '& training_blocks==', '& degradation_blocks==', '& trials_per_block==', '& dec_temp =='],dtype='str')\n",
    "vals = np.array([switch, contingency_degr, reward_naive, q, h,\\\n",
    "                 training_blocks, degradation_blocks, trials_per_block, dec_temp], dtype='str')\n",
    "whole_query = np.char.join('', np.char.add(strs, vals))\n",
    "base_query = ' '.join(whole_query.tolist())\n",
    "base_df = df.query(base_query)\n",
    "# base_df_dkl = df_dkl.query(base_query)\n",
    "p = queries[0]\n",
    "context_plot_cue_dependent(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DKL plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "one_run = True\n",
    "for p in [p]:\n",
    "    print(p)\n",
    "    plot_df = base_df_dkl.query('t==3' + ' & p ==' + str(p))\n",
    "    fig, axes = plt.subplots(ncols=4, nrows=1, figsize=(20,3))\n",
    "    for cont in range(4):\n",
    "\n",
    "        # print('context_cues == ' + str(cue) + ' & context== ' + str(cont))\n",
    "        if one_run == True:\n",
    "            quer = 'context== ' + str(cont) + ' & run == 0'\n",
    "        else:\n",
    "            quer = 'context== ' + str(cont)\n",
    "        if cont == 3:\n",
    "            sns.lineplot(ax=axes[cont], data=plot_df.query(quer),\n",
    "                        x='trial', y='avg_dkl', hue='h', legend=True, \\\n",
    "                        palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size))\n",
    "            print(np.unique(plot_df['h']))\n",
    "            axes[cont].legend(ncol = np.unique(plot_df['h']).size, bbox_to_anchor=(-3.3, -0.3), loc='upper left',\\\n",
    "                borderaxespad=0,title='h')\n",
    "        else:\n",
    "            \n",
    "            sns.lineplot(ax=axes[cont], data=plot_df.query(quer),\n",
    "                        x='trial', y='avg_dkl', hue='h',legend=False,\\\n",
    "                            palette=sns.color_palette('Blues_r',n_colors=np.unique(plot_df['h']).size))\n",
    "        axes[cont].set_title('context: ' + str(cont))\n",
    "\n",
    "    ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "    cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "    for ax in axes.flatten():    \n",
    "        for i, row in ranges.iterrows():\n",
    "            ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    "\n",
    "plt.figure()\n",
    "sns.histplot(data=plot_df.query('t==0 & trial_type == 1 & cue = 0', x ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimality only during extinction for different context ambiguities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "ps = [0.6,0.7, 0.8]\n",
    "\n",
    "\n",
    "plot_df = base_df.query('trial_type > 1 & trial_type <= 2 &t==0')\n",
    "plot_df = plot_df.astype({'h': 'category'})\n",
    "print(plot_df.size)\n",
    "grouped = plot_df.groupby(by=['agent', 'run','h','cue'])\n",
    "plot_df['policy_optimality_subset'] = grouped['chose_optimal'].transform('cumsum')\n",
    "plot_df['offset'] = grouped['ith_cue_trial'].transform('min')\n",
    "plot_df['policy_optimality_subset'] = plot_df['policy_optimality_subset'] / (plot_df['ith_cue_trial'] - plot_df['offset']+1)\n",
    "\n",
    "reward_naive = [0,1]\n",
    "cue_degradation = [1,0]\n",
    "# this is a bit confusing\n",
    "ttls = ['Policy optimality during degradation and extinction when NOT reward naive',\\\n",
    "        'Policy optimality during degradation and extinction when reward naive']\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize = (15,7))\n",
    "plt.tight_layout()\n",
    "for cue in [0,1]:\n",
    "    for pi,p in enumerate(ps):\n",
    "        if pi == 2 and cue==1:\n",
    "            sns.lineplot(ax = axes[cue,pi], data=plot_df.query('learn_rew == 0'  + ' & p==' + str(p) + ' & cue==' + str(cue)),\\\n",
    "                        x = 'trial',y='policy_optimality_subset', hue='h', legend=True)\n",
    "            axes[cue,pi].set_title('p: ' + str(p) + ' cue: ' + str(cue))\n",
    "        else:\n",
    "            sns.lineplot(ax = axes[cue,pi], data=plot_df.query('learn_rew == 0'  + ' & p==' + str(p) + ' & cue==' + str(cue)),\\\n",
    "                        x = 'trial',y='policy_optimality_subset', hue='h', legend=False)\n",
    "            axes[cue,pi].set_title('p: ' + str(p) + ' cue: ' + str(cue))\n",
    "\n",
    "    ranges = plot_df.groupby('trial_type')['trial'].agg(['min', 'max'])\n",
    "    cols = [[1,1,1], [0,0,0],[1,1,1]] \n",
    "    for ax in axes.flatten():\n",
    "        for i, row in ranges.iterrows():\n",
    "            ax.axvspan(xmin=row['min'], xmax=row['max'], facecolor=cols[i], alpha=0.05)\n",
    " \n",
    "    ax.legend(bbox_to_anchor=(-2,-0.3), loc='upper left', borderaxespad=0,title='h',ncol=np.unique(plot_df['h'].size))\n",
    "    # fig.suptitle(ttls[f] + ' and cue_degradation: ' + str(cue_degradation[ci]) , fontsize=15, y=1.08)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation plot for context beliefs\n",
    "\n",
    "\n",
    "Top panel shows posterior over contexts at t==3;</br>\n",
    "bottom 3 panels shows policy, prior, likelihood and posterior;</br>\n",
    "</br>\n",
    "Legend is shit because idk how to control it but basically:\n",
    "</br>\n",
    "</br>\n",
    "top panel: blue = h1, red = h100</br>\n",
    "bottom panels: blue solid - h1, training context;</br>\n",
    "               blue solid - h1, extinction context;</br>\n",
    "               red solid - h100, training context;</br>\n",
    "               red solid - h100, extinction context;</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load actual dataframe\n",
    "\n",
    "nc = 4\n",
    "extinguish = True\n",
    "h =  [1,100]\n",
    "cue_ambiguity = [0.8]                       \n",
    "context_trans_prob = [1/nc]                \n",
    "degradation = [True]\n",
    "cue_switch = [False]\n",
    "reward_naive = [False]\n",
    "training_blocks = [4]\n",
    "degradation_blocks=[4]\n",
    "trials_per_block=[70]\n",
    "arrays = [cue_switch, degradation, reward_naive, context_trans_prob, cue_ambiguity,h,\\\n",
    "        training_blocks, degradation_blocks, trials_per_block]\n",
    "\n",
    "names = load_file_names(arrays)\n",
    "df_context = load_df_animation_context(names)\n",
    "df_policy = load_df_animation_pol(names)\n",
    "print(np.unique(df_policy['h']).size)\n",
    "df_policy = df_policy.query('run == 0')\n",
    "cntxts = ['(context == 0 | context == 2)', '(context == 1 | context == 3)']\n",
    "\n",
    "\n",
    "\n",
    "### prepare figure plotting \n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def animation_figure():\n",
    "    ##### figure 1\n",
    "    fig1 = px.line(plot_df, x='policy', y='policy_prior',animation_frame='trial',animation_group='context',\\\n",
    "        color=\"h\",line_dash = 'context', range_y=[0,1])\n",
    "    fig2 = px.line(plot_df, x='policy', y='policy_like',animation_frame='trial',animation_group='context',\\\n",
    "        color=\"h\",line_dash = 'context', range_y=[0,1])\n",
    "    fig3 = px.line(plot_df, x='policy', y='policy_post',animation_frame='trial',animation_group='context',\\\n",
    "        color=\"h\",line_dash = 'context', range_y=[0,1])\n",
    "    fig4 = px.line(base_df.query(trial_que + ' & t==1' + ' & policy==0'),\\\n",
    "        x='context', y='post_context',animation_frame='trial',animation_group='h', color=\"h\", range_y=[0,1])\n",
    "    fig4\n",
    "\n",
    "    # ##### combine\n",
    "\n",
    "    # integrate the two figures, putting second figure on separate axis\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=[t for t in fig1.data] \n",
    "            + [t.update(xaxis=\"x2\", yaxis=\"y2\") for t in fig2.data] \n",
    "            + [t.update(xaxis=\"x3\", yaxis=\"y3\") for t in fig3.data] \n",
    "            + [t.update(xaxis=\"x4\", yaxis=\"y4\") for t in fig4.data],\n",
    "        frames=[\n",
    "            go.Frame(\n",
    "                name=fr1.name,\n",
    "                data=[t for t in fr1.data]\n",
    "                + [t.update(xaxis=\"x2\", yaxis=\"y2\") for t in fr2.data]\n",
    "                + [t.update(xaxis=\"x3\", yaxis=\"y3\") for t in fr3.data]\n",
    "                + [t.update(xaxis=\"x4\", yaxis=\"y4\") for t in fr4.data],\n",
    "            )\n",
    "            for fr4, fr3, fr2, fr1 in zip(fig4.frames, fig3.frames, fig2.frames, fig1.frames)\n",
    "        ],\n",
    "        layout=fig1.layout,\n",
    "    )\n",
    "\n",
    "\n",
    "    # now config axes appropriately\n",
    "    fig.update_layout(\n",
    "        xaxis_domain=[0, 0.3],\n",
    "        yaxis_domain=[0, 0.45],\n",
    "        yaxis_range=[0, 1],\n",
    "\n",
    "        xaxis2={\"domain\":[0.33, 0.63], \"matches\": None, \"title\":{\"text\":fig2.layout.xaxis.title.text}},\n",
    "        yaxis2={\"range\": [0,1], \"domain\":[0, 0.45],'position':0.33, \"matches\":None,\"title\":{\"text\":fig2.layout.yaxis.title.text}},\n",
    "\n",
    "        xaxis3={\"domain\": [0.66, 1], \"matches\": None, \"title\":{\"text\":fig3.layout.xaxis.title.text}},\n",
    "        yaxis3={\"range\": [0,1],\"domain\":[0, 0.45], \"matches\": None,'position':0.66, \"title\":{\"text\":fig3.layout.yaxis.title.text}},\n",
    "        xaxis4={\"domain\": [0, 0.5], \"matches\": None,\"showticklabels\":False},\n",
    "        yaxis4={\"range\": [0,1], \"domain\":[0.5, 1], \"matches\": None, \"title\":{\"text\":fig4.layout.yaxis.title.text}},\n",
    "\n",
    "        showlegend=True,\n",
    "    )\n",
    "\n",
    "    for i, frame in enumerate(fig.frames):\n",
    "        frame.layout.title = \"Cue: {}, Trial type: {}, True optimal:{}\".format(cue, trial_type[i],true_optimal[i])\n",
    "    for step in fig.layout.sliders[0].steps:\n",
    "        step[\"args\"][1][\"frame\"][\"redraw\"] = True\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "### select parameters for figure plotting\n",
    "### cue is most important here and you can change it to get an idea for different cues\n",
    "\n",
    "\n",
    "cue = 1\n",
    "switch = False\n",
    "contingency_degr = degradation[0]\n",
    "reward_naive = reward_naive[0]\n",
    "q = 0.25\n",
    "trials_per_block = 70\n",
    "training_blocks = training_blocks[0]\n",
    "degradation_blocks = degradation_blocks[0]\n",
    "p = cue_ambiguity[0]\n",
    "\n",
    "strs = np.array(['switch_cues == ', '& contingency_degradation==', '& learn_rew==', '& q==', '& h>=',\\\n",
    "                 '& training_blocks == ', '& degradation_blocks == ', '& trials_per_block ==', '& p =='],dtype='str')\n",
    "vals = np.array([switch, contingency_degr, reward_naive, q, 1,\\\n",
    "                 training_blocks, degradation_blocks, trials_per_block, p], dtype='str')\n",
    "whole_query = np.char.join('', np.char.add(strs, vals))\n",
    "base_query = ' '.join(whole_query.tolist())\n",
    "base_df = df_policy.query(base_query)\n",
    "trial_type = plot_df.groupby(by=['trial'])['trial_type'].mean().to_numpy().astype('int32')\n",
    "true_optimal = plot_df.groupby(by=['trial'])['true_optimal'].mean().to_numpy().astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "### here you can change which trial type you want to look at\n",
    "\n",
    "trial_que = 'trial_type == 0  & context_cues ==' + str(cue)     # training\n",
    "trial_que = 'trial_type == 1  & context_cues ==' + str(cue)   # degradation\n",
    "# trial_que = 'trial_type == 2  & context_cues ==' + str(cue)   # extinction\n",
    "\n",
    "plot_df = base_df.query(trial_que + ' & t==0' + ' &' + cntxts[cue])\n",
    "trial_type = plot_df.groupby(by=['trial'])['trial_type'].mean().to_numpy().astype('int32')\n",
    "true_optimal = plot_df.groupby(by=['trial'])['true_optimal'].mean().to_numpy().astype('int32')\n",
    "animation_figure()\n",
    "\n",
    "\n",
    "trial_que = 'trial_type == 1  & context_cues ==' + str(cue)   # degradation\n",
    "plot_df = base_df.query(trial_que + ' & t==0' + ' &' + cntxts[cue])\n",
    "trial_type = plot_df.groupby(by=['trial'])['trial_type'].mean().to_numpy().astype('int32')\n",
    "true_optimal = plot_df.groupby(by=['trial'])['true_optimal'].mean().to_numpy().astype('int32')\n",
    "animation_figure()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4cff3abf1678755e0069fd79299a535fe1940bcd71a6b01d9f4386710b2b163f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
